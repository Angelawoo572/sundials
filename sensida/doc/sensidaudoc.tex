% LaTeX source for SensIDA paper.

\input{prtgrph}

\documentstyle[12pt]{siam}

\setlength{\oddsidemargin}{0in}   %These seem to be necessary
\setlength{\evensidemargin}{0in}  %   to get equal side margins
\setlength{\textwidth}{6.5in}     %Default is 6.15in

\def\drop#1 {\ }

\newcommand{\fortran}{{\sc Fortran}}
\newcommand{\C}{{\sc C}}
\newcommand{\uu}{{\bf u}}
\newcommand{\ff}{{\bf f}}
\newcommand{\boldF}{{\bf F}}
\newcommand{\bfF}{{\bf F}}
\newcommand{\bfR}{{\bf R}}
\newcommand{\pbar}{\overline{p}}
\newcommand{\deltat}{h}
\newcommand{\dFdy}{\frac{\partial F}{\partial y}}
\newcommand{\dFdyp}{\frac{\partial F}{\partial y^{\prime}}}
\newcommand{\dFdY}{\frac{\partial F}{\partial Y}}
\newcommand{\dGdY}{\frac{\partial G}{\partial Y}}
\newcommand{\dFdpi}{\frac{\partial F}{\partial p_i}}
\newcommand{\dfdpi}{\frac{\partial f}{\partial p_i}}
\newcommand{\atol}{{\sc atol}}
\newcommand{\rtol}{{\sc rtol}}
\newcommand{\floor}{{\sc floor}}
\newcommand{\rhomax}{\rho_{\max}}
\newcommand{\sit}{s_{i}(t)}
\newcommand{\sprimei}{s_{i}^{\prime}}
\newcommand{\sprimeit}{s_{i}^{\prime}(t)}
\newcommand{\wit}{w_{i}(t)}
\newcommand{\wprime}{w^{\prime}}
\newcommand{\wprimei}{w_{i}^{\prime}}
\newcommand{\wprimeit}{w_{i}^{\prime}(t)}
\newcommand{\yprime}{y^{\prime}}
\newcommand{\Yprime}{Y^{\prime}}
\newcommand{\yprimet}{y^{\prime}(t)}
\newcommand{\Yprimet}{Y^{\prime}(t)}

\title{User Documentation for SensIDA, a variant of IDA for
Sensitivity Analysis%
\thanks{
This work was performed under the auspices of the U.S. Department of
Energy by University of California Lawrence Livermore National
Laboratory under contract No. W-7405-Eng-48.}}

\author{Steven L. Lee
\and
Alan C. Hindmarsh
\thanks{Center for Applied Scientific Computing, 
Lawrence Livermore National Laboratory, Livermore, CA 94551}}

\hyphenation{Sens-IDA}
\hyphenation{Sens-PVODE}
\hyphenation{Sens-IDA-SPGMR}
\hyphenation{Sens-IDA-Malloc}
\hyphenation{N-VECTOR}

\begin{document}

\maketitle


\section{Introduction}

SensIDA and IDA~\cite{IDA_UG} are general-purpose codes for solving
differential-algebraic equation (DAE) initial value problems.
SensIDA is a variant of IDA that includes options for simultaneously
computing the DAE solution together with its first-order sensitivity
coefficients with respect to model parameters.
SensIDA is written in ANSI-standard C and it is mainly based on IDA,
DASPK3.0~\cite{DASPK3}, and SensPVODE~\cite{SensPVODE_UG}.
IDA is based on DASPK2.0~\cite{DASPK2}.
DASPK3.0 is a Fortran77 code for the sensitivity analysis of DAE
initial value problems.
SensPVODE is a sensitivity analysis variant of the parallel ordinary
differential equation solver PVODE~\cite{PVODE_UG}.

SensIDA can be compiled to run on serial or parallel computers.
This is accomplished by specifying that the serial or parallel
version of the vector module {\tt NVECTOR} is used when compiling SensIDA.
The parallel version of SensIDA uses MPI (Message-Passing
Interface~\cite{MPI}) to achieve parallelism, and is intended for
a distributed Single Program Multiple Data environment in which all
vectors are identically partitioned across processors.
The idea is for each processor to solve a certain fixed subset of the
DAEs that describe the model problem and the first-order sensitivity
coefficients of the solution.

SensIDA includes all of the numerical methods contained in IDA:
backward differentiation formulas (BDF) for time integration;
Newton/direct methods or an Inexact Newton/Krylov method for solving
the nonlinear systems; preconditioning modules for Krylov subspace
methods such as GMRES~\cite{GMRES}.
The linear solver and preconditioning modules allow for other direct
methods, Krylov methods, and user-supplied preconditioners to be
easily included.
SensIDA also retains the use of matrix-free methods~\cite{BH89}
for approximating preconditioned matrix-vector products. 
This approach obtains matrix-vector products within GMRES without
explicitly computing or storing the linear system matrix.
The parallel version of SensIDA includes only the Krylov method GMRES
for solving linear systems.
SensIDA was developed and tested on a cluster of Sun-SPARC
workstations.

The remainder of this document is organized as follows:
Section 2 sets the mathematical notation and summarizes the basic
approach to sensitivity analysis.
Section 3 summarizes the organization of the SensIDA solver, while
Section 4 summarizes its usage.
Section 5 describes three example problems.
Finally, Section 6 discusses the availability of SensIDA.

\section{Mathematical Considerations}

In many large-scale computational simulations, the governing equations
can often be spatially discretized and then numerically solved as a
system of DAEs.
Typically, these equations contain parameter values (for example,
reaction rates and problem coefficents) that are not precisely known.
In addition to numerically solving the DAEs, it may be desirable to
determine the sensitivity of the results with respect to the model
parameters.
Such sensitivity information is useful because it indicates which
parameters are most influential in affecting the behavior of the
simulation.
	
SensIDA is a variant of IDA that computes the first-order sensitivity
of the DAE solution with respect to some or all of its model
parameters.
When computing sensitivities in this context, one is interested in
solving the DAE initial value problem
\begin{equation}
F(t, y, \yprime, p)=0 \label{orig_DAE}
\end{equation}
where $y$, $\yprime$ and $F$ are vectors in $\bfR^N$, 
$p$ is a vector of parameters in $\bfR^m$, and
$t$ is the independent variable.
The first-order {\em sensitivities} are defined as
\[
\sit = \frac{\partial y(t,p)}{\partial p_i}, \; i = 1,\cdots, m,
\]
and the sensitivity equations are obtained by
differentiating~(\ref{orig_DAE}) with respect to each parameter $p_i$:
\begin{equation}
\frac{\partial F}{\partial y}s_i + \frac{\partial F}{\partial
\yprime}\sprimei + \frac{\partial F}{\partial p_i} = 0, \; 
i = 1,\cdots, m. \label{sensres}
\end{equation}
If $\pbar_i$ is a nonzero constant, equations for the {\em scaled}
sensitivities
\begin{equation}
w_i(t) = \pbar_i s_i(t)
\end{equation}
can be obtained by multiplying each equation~(\ref{sensres})
by~$\pbar_i$:
\begin{equation}
\frac{\partial F}{\partial y}w_i + \frac{\partial F}{\partial
\yprime}\wprimei + \pbar_i \frac{\partial F}{\partial p_i} = 0, \; 
i = 1,\cdots, m. \label{scaledsensres}
\end{equation}
%This scaling is done in order to obtain variables $w_i$ with the same
%physical units as $y_i$.
This scaling is done in order to obtain vectors $w_i$ with the same
physical units as the components in $y$.

SensIDA carries out the time integration of the combined system
(\ref{orig_DAE}) and (\ref{scaledsensres}) by viewing it as a DAE system
of size $N(m+1)$.
By defining
\[
Y(t) = \left( \begin{array}{c}
y(t) \\ w_1(t) \\ \vdots \\ w_m(t) \end{array} \right)
\;\; \mbox{ and } \; \;
\bfF(t, Y, \Yprime, p) =  \left( \begin{array}{c}
F(t, y, \yprime, p) \\ 
\dFdy w_1(t) + \dFdyp \wprime_{1}(t) + \pbar_1 \frac{\partial F}{\partial p_1} \\ 
\vdots \\ 
\dFdy w_m(t) + \dFdyp \wprime_{m}(t) + \pbar_m \frac{\partial F}{\partial p_m} 
\end{array} \right),
\]
the combined DAE system is simply
\begin{equation}
{\bfF}(t,Y,\Yprime,p)=0
\label{boldF}
\end{equation}
and the initial conditions
\[
Y(t_0) = Y_0(p),~~~\; \Yprime(t_0) = \Yprime_0(p)
\]
must be consistent so that they satisfy~(\ref{boldF}).
Given initial guesses for $Y(t_0)$ and $\Yprime(t_0)$, SensIDA
includes a routine that computes consistent initial conditions for
certain classes of DAE problems~\cite{BHP98}.

SensIDA uses backward differentiation formulas (BDFs) of order 1
through 5 to integrate the system~(\ref{boldF}).
The first-order case is the backward Euler method, and in that case
this approach yields the nonlinear system
\begin{equation}
{\bf 0} = G(Y_{n+1}) \equiv 
\boldF(t_{n+1},Y_{n+1},\frac{Y_{n+1}-Y_{n}}{h},p)
\label{G_eqns}
\end{equation}
where $h = t_{n+1}-t_{n}$ is the current stepsize.
Due to the form of $\boldF$, the Jacobian matrix $\partial G/\partial Y$
has the lower block triangular structure
\[
\dGdY =
\left( \begin{array}{cccc}
J & & &  \\
J_1 & J &  & \\
\vdots &  & \ddots & \\
J_m &  & & J \end{array} \right)
\]
where 
\[
%J = \dFdy \;\; \mbox{ and } \;\;
J = \frac{1}{h} \dFdyp + \dFdy \;\; \mbox{ and } \;\;
J_i = \frac{\partial}{\partial y} \left(\dFdy w_i + \dFdyp \wprime_i +
\pbar_i \frac{\partial F}{\partial p_i}\right).
\]
Higher-order BDFs also yield Jacobian matrices~$\partial G/\partial Y$
with this same lower block triangular structure and with identical
block-diagonal entries, $J$.
The only difference is that the coefficient~$1/h$
becomes~$\alpha_0/h$, where $\alpha_0$ is a coefficient in the
BDF.
SensIDA solves the nonlinear systems~$G(Y_{n+1})={\bf 0}$ by using
the simultaneous corrector method~\cite{MP96}, a technique in which
the Newton iteration uses the block-diagonal portion of~$\partial
G/\partial Y$ as the linear system matrix.
This results in a decoupling that allows~$J$ to be used
repeatedly in solving the $(m+1)$ linear systems that arise: one linear
system for the Newton correction to the state variables $y$, and~$m$ linear
systems for the corrections to the~$m$ scaled sensitivity vectors $w_i$.
Because all of the Jacobian matrices are identical, the latter systems are
solved using the same preconditioner and/or linear system solver that
were specified for the original DAE problem~(\ref{orig_DAE}).
%In particular, the SPGMR (scaled, preconditioned GMRES) method is
%used when solving stiff initial-value problems via BDF methods.

The integrator computes an estimate $E_{n}$ of the local error at each
time step, and strives to satisfy the inequality
\begin{equation}
\left\| E_n\right\|_{rms,W} < 1 ~.  \label{Err}
\end{equation}
Here the weighted root-mean-square (rms) norm is defined by
\begin{equation}
\left\| E_n\right\|_{rms,W}=\left[\sum_{i=1}^{N(m+1)}\frac{1}{N(m+1)}
\left(W_i E_{n,i}\right) ^2\right] ^{1/2}  \label{rms}
\end{equation}
where $E_{n,i}$ denotes the $i$th component of $E_n$, and the $i$th 
component of the weight vector is 
\begin{equation}
W_i=\frac{1}{\rtol |Y_i|+\atol_i} ~.  \label{weight}
\end{equation}
This permits an arbitrary combination of relative and absolute error control.
The user-specified relative error tolerance is the scalar $\rtol$; the
user-specified absolute error tolerance is $\atol$, which may be a
scalar or a vector.
The value for $\rtol$ indicates the number of digits of relative
accuracy for a single time step.
The specified value for $\atol_{i}\;$ indicates the values of the
corresponding component of the solution vector which may be thought of
as being zero, or at the noise level.
In particular, if we set $\atol_i=\rtol \times \floor_i$ then
$\floor_i$ represents the floor value for the $i$th component of the
solution.
The magnitude of~$\floor_i$ is the value for which there is a 
crossover from relative error control to absolute error control.
In the case of vector absolute tolerances, a typical default for the
scaled sensitivity vectors is to use the same $\atol$ as for the state
variables $y$.
Since the tolerances define the allowed error per step, they should
be chosen conservatively.
Experience indicates that a conservative choice yields a more
economical solution than error tolerances that are too large.

For estimating the scaled sensitivity residual~(\ref{scaledsensres}), 
SensIDA has an option that applies centered
differences separately:
\begin{equation}
\dFdy w_i + \dFdyp \wprime_i \approx
\frac{F(t,y+\delta_y w_i,\yprime+\delta_y \wprime_i,p)-
F(t,y-\delta_y w_i,\yprime-\delta_y \wprime_i,p)}{2 \delta_y}
\label{yterms}
\end{equation}
and
\begin{equation}
\pbar_i \dFdpi \approx
\frac{F(t,y,\yprime,p+\delta_i \pbar_i e_i)-
F(t,y,\yprime,p-\delta_i \pbar_i e_i)}{2 \delta_i}.
\label{pterm}
\end{equation}
As is typical for finite differences, the proper choice of
perturbations $\delta_y$ and $\delta_i$ is a delicate matter.
SensIDA uses a choice that takes into account
several problem-related features;
namely, the relative DAE error tolerance
$\rtol$, the machine unit roundoff $\epsilon_{\rm machine}$,
and the weighted root-mean-square norm of the scaled sensitivity
$w_i$. 
We then define
\begin{equation}
\delta_i = \sqrt{\max(\rtol, \epsilon_{\rm machine})}
~~~{\rm and}~~~
\delta_y = \frac{1}{\max(1/\delta_i, \|w_i\|_{rms})}.
\end{equation}
The norm for $\|w_i\|$ uses the $\rtol$ and $\atol_i$ associated with
the state variables $y$.
The terms $\epsilon_{\rm machine}$ and $1/\delta_i$ are included as
divide-by-zero safeguards in case~$\rtol = 0$ or~$||w_i|| = 0$.
Roughly speaking (i.e., if the safeguard terms are ignored),
$\delta_i$ gives a $\sqrt{\rtol}$ relative perturbation to parameter
$i$, and $\delta_y$ gives a unit weighted rms norm perturbation to~$y$.
Of course, the main drawback of this approach is that it requires four
evaluations of $F(t,y,\yprime,p)$.

Another centered differences technique for estimating the scaled
sensitivity residuals uses
\begin{equation}
%\dFdy w_i + \dFdyp \wprime_i + \pbar_i \dFdpi \approx
\frac{F(t,y+\delta w_i,\yprime+\delta \wprime_i,p+\delta \pbar_i e_i)-
F(t,y-\delta
w_i,\yprime-\delta \wprime_i,p-\delta \pbar_i e_i)}{2 \delta}
\label{allterms}
\end{equation}
in which
$$
\delta = \min(\delta_i, \delta_y).
$$
If $\delta_i = \delta_y$, a Taylor series analysis shows
that the sum of~(\ref{yterms})--(\ref{pterm})
and~(\ref{allterms}) are equivalent to within $O(\delta^2)$.
The latter approach, however, is half as costly since it only requires
two evaluations of $F(t,y,\yprime,p)$.  
To take advantage of this savings, it may also be desirable to
use~(\ref{allterms}) when $\delta_i \approx \delta_y$.
SensIDA accommodates this possibility by allowing the user to
specify a threshold parameter~$\rhomax$.
In particular, if $\delta_i$ and $\delta_y$ are within a factor of
$|\rhomax|$ of each other, then~(\ref{allterms}) is used to estimate the
scaled sensitivity residuals.
Otherwise, the sum of (\ref{yterms})--(\ref{pterm}) is used since
$\delta_i$ and $\delta_y$ differ by a relatively large amount and the
use of separate perturbations is prudent.

These procedures for choosing the perturbations ($\delta_i$,
$\delta_y$, $\delta$) and switching ($\rhomax$) between centered
difference formulas have also been implemented for first-order,
forward difference formulas as well.
In the latter case, forward differences can be applied separately or
the single forward difference
\begin{equation}
\dFdy w_i + \dFdyp \wprime_i + 
\pbar_i \dFdpi \approx
\frac{F(t,y+\delta w_i, \yprime+\delta \wprime_i, p + \delta \pbar_i e_i)-
F(t,y,\yprime,p)}{\delta}
\label{switching}
\end{equation}
can be used.
In SensIDA, the default value of $\rhomax=0$ indicates the use of
the centered difference~(\ref{allterms}) exclusively.
Otherwise, the magnitude of $\rhomax$ and its sign (positive or
negative) indicates whether this switching is done with regard to
(centered or forward) finite differences, respectively.

In contrast to the above notation used in describing the mathematical
details, the sections that follow use new variable names in
explaining the organization, usage, and example programs of SensIDA.
For convenient reference, we define these names as follows:
\begin{itemize}
\item {\tt res} is the name of the user-supplied function that 
computes the DAE residual $F(t,y,\yprime,p)$.
\item {\tt Ny} is the number of DAEs contained in {\tt res} ($=N$
above)
\item {\tt Ns} is the number of sensitivity vectors $w_i$ to be
computed ($=m$ above)
\item {\tt Np} is the number of parameters contained in $p$ ($\geq$
{\tt Ns})
\item {\tt Ntotal} is the total number of DAEs to be solved by SensIDA
($=$ {\tt (1+Ns)Ny})
\item {\tt yy} is the vector of length ${\tt Ntotal}$ that contains
the {\tt Ny} differential and algebraic variables and {\tt Ns} scaled
sensitivity vectors $w_i$
\item {\tt rhomax} is the finite difference threshold parameter
$\rhomax$ \\
For estimating the residual of the scaled sensitivity equations~(\ref{scaledsensres}), 
the values and meanings for {\tt rhomax} are as follows:
\begin{itemize}
\item [$\diamond$] {\tt rhomax = 0:} 
Use the centered difference~(\ref{allterms}).
\item [$\diamond$] {\tt 0 < rhomax < 1:} 
Use the sum of the centered differences~(\ref{yterms}) and~(\ref{pterm}).
\item [$\diamond$] {\tt rhomax >= 1:}
If $\delta_y$ and $\delta_i$ are within a factor of {\tt rhomax} of
each other, then~(\ref{allterms}) is used.
Otherwise, the sum of the centered differences~(\ref{yterms}) and
(\ref{pterm}) is used.
\item [$\diamond$] {\tt -1 < rhomax < 0:} 
Use the sum of~(\ref{yterms}) and~(\ref{pterm}), with forward
differences instead of centered differences.
\item [$\diamond$] {\tt rhomax <= -1:}
If $\delta_y$ and $\delta_i$ are within a factor of {\tt |rhomax|} of
each other, then use~(\ref{switching}).
Otherwise, use the sum of~(\ref{yterms}) and~(\ref{pterm}) with
forward differences instead of centered differences.
\end{itemize}
\end{itemize}

Lastly, SensIDA provides a way to compute the sensitivities of $y$
with respect to a certain subset of the {\tt Np} parameters.
For instructions on specifying which {\tt Ns} parameters are to be studied,
see the description of {\tt plist} in the next section.

\section{Code Organization}

One way to visualize SensIDA is to think of the code as being
organized in layers, as partially shown in Fig.~\ref{fig-sensidaorg}.
The user's main program resides at the top level.
The main program creates the required data structures, makes various
initializations, defines the DAE residual {\tt res}, defines the
preconditioner setup and solve routines (if any), and makes calls to
various modules at the second level. The main program also manages
input/output.

At the second level, the {\tt SENSIDA} module contains several routines
that can be called by the user: {\tt SensIDAMalloc}, for memory allocation
and basic initializations related to sensitivity analysis; 
{\tt SensIDAFree}, for memory deallocation; and {\tt SensSetVecAtol},
for the vector case of setting absolute error tolerances for
sensitivities.
The {\tt SENSIDA} routine {\tt RESDQ} is called by the {\tt IDA}
module.
{\tt RESDQ} is responsible for computing the DAE residual by
calling the user's {\tt res} routine to evaluate $F(t, y, \yprime, p)$
and for using various finite difference formulas to estimate the DAE
scaled sensitivity residuals.
By design, the {\tt SENSIDA} module is independent of the choice of
linear solver method used within the Newton iteration.

At the third level are the modules for the linear system solver, which
at present are: {\tt SensIDASPGMR}, {\tt SensIDABAND}, {\tt
SensIDADENSE}.
Each of these provides an interface to a corresponding generic solver
for linear systems by a SPGMR, banded, or dense algorithm.
In each case, the linear solver is called to solve the {\tt (1+Ns)}
linear systems of size {\tt Ny}. 
The SPGMR method consists of the modules {\tt SPGMR} and
{\tt ITERATIV}.
{\tt SensIDASPGMR} also accesses the user-supplied preconditioner
solver routine, if specified, and possibly a user-supplied
routine that computes and preprocesses the preconditioner by way of
the Jacobian matrix or an approximation to it.
The direct method modules, {\tt SensIDABAND} and {\tt SensIDADENSE},
access the user's Jacobian routine {\tt jac} if one is supplied.

Finally, at the second level, the routine {\tt IDASolve} within the
{\tt IDA} module is used to manage the time integration.
{\tt IDASolve} makes calls to {\tt SENSIDA} to evaluate the scaled
sensitivity residuals, and calls the linear solver module
{\tt SensIDADENSE}, {\tt SensIDABAND}, or {\tt SensIDASPGMR} to solve the linear
systems that arise at each Newton iteration.

\begin{figure}[p]
\VGPrint{sensidaorg.ps}{5.5}{8}{1.4}{1.5}{0.75}
\vspace{2.0in}
\caption{Overall structure of the SensIDA package.
Modules comprising the central solver are distinguished by rounded
boxes, while the user program, linear solver, and auxiliary
modules are in unrounded boxes.} 
\label{fig-sensidaorg}
\end{figure}

The following modules reside below the levels just described.
The {\tt LLNLTYPS} module defines types {\tt real}, {\tt integer}, and {\tt
boole} (boolean), and facilitates changing the precision of the
arithmetic in the package from double to single, or the reverse.
The {\tt LLNLMATH} module specifies power functions and provides a function
to compute the machine unit roundoff.
Finally, we now describe the {\tt NVECTOR} module.

In creating SensIDA from IDA, we developed a sensitivity
version of the {\tt NVECTOR} module.
A revised {\tt NVECTOR} module is needed because the overall DAE
system has length {\tt Ntotal}, but it consists of 1+{\tt Ns}
DAE subsystems of length {\tt Ny};
namely, the original nonlinear DAE system~(\ref{orig_DAE}) and
{\tt Ns} scaled sensitivity residuals~(\ref{scaledsensres}).
Several steps are involved in partitioning and distributing
the subsystems in a multiprocessor environment.
First, each processor is responsible for solving a contiguous portion
of each subsystem, of length {\tt Nlocal}.
Note that {\tt Nlocal} need not be the same for each processor;
however, the sum of all the {\tt Nlocal} values must be {\tt Ny}.
Furthermore, the {\tt 1+Ns} subsystems of size {\tt Ny} are
identically partitioned among the processors.
This implementation is handled through the revised {\tt NVECTOR}
module and its use of abstract data types: 
{\tt type machEnvType}, for the machine environment data block (e.g.,
{\tt Nlocal}); and
{\tt type N\_Vector}, a data structure for the partitioned and
distributed vectors just described.
To achieve parallelism for any vector operation, each processor
performs the operation on its assigned portions of the input vectors,
followed by a global reduction where needed.
In this way, vector calculations can be performed simultaneously with
each processor working on its own block-portions of the vector.

The parallel version of SensIDA uses MPI (Message Passing
Interface~\cite{MPI}) for all inter-processor communication. 
This achieves a high degree of portability, since MPI is becoming
widely accepted as a standard for message passing software.
For a different parallel computing environment, some rewriting of the
vector module could allow the use of other specific machine-dependent
instructions.

\section{Using SensIDA}

This section describes the use of SensIDA.
We begin with a brief overview, in the form of a skeleton user
program for parallel applications.
Following that are detailed descriptions of the interface to the
various user-callable routines, and of the user-supplied routines.
We also describe a preconditioner module that is part of the IDA package.
Finally, there are comments on usage under C++.

\subsection{Overview of Usage}
\label{Usage}

The following is a skeleton of the user's main program (or calling
program) as an application of SensIDA on a parallel machine.
The user program is to have these steps in the order indicated.
In the serial case, Steps 3, 5, 17, and 18 can be omitted.
For the sake of brevity, we defer many of the details to the later
subsections, or to the IDA user document~\cite{IDA_UG}.

\begin{enumerate}

\item {\tt \#include} header files needed, to obtain various type
definitions, enumerations, macros, etc.  
The files include {\tt llnltyps.h}, {\tt llnlmath.h}, {\tt ida.h},
{\tt nvector.h}, {\tt mpi.h}; one or more of the files {\tt
idadense.h}, {\tt idaband.h}, {\tt idaspgmr.h}, {\tt idabbdpre.h}
associated with the choice of preconditioner and/or linear system
solvers; and {\tt sensida.h}.
Also, the calling program must set the integer variables {\tt Ny, Np,
Ns} and {\tt Ntotal}.

\item The calling program must define a pointer to a user-defined data
block that is passed to the user's {\tt res} routine. 
This data block must include a real pointer (for example, {\tt p})
that points to the array of real parameters used by {\tt res} to
evaluate $F(t,y,\yprime,p)$. 
For example, if the pointer to the data block has the form 
\newline
{\tt typedef struct \{\ldots, real *p\} *rdata}, then {\tt rdata->p = p}
must point to the real array in which {\tt p[i-1] =} $p_i$, for
%$i=1,\ldots,m$.
{\tt i=1,...,Np}.

\item {\tt MPI\_Init(\&argc, \&argv)} to initialize MPI if used by
the user's program.  Here {\tt argc} and {\tt argv} are the command
line argument counter and array received by {\tt main}.

\item Set {\tt Nlocal = } the local vector length
for this processor, and
{\tt Ny = } the global vector length for this processor.
Note that {\tt Ny} is the sum of all values of {\tt Nlocal}.

\item {\tt machEnv = PVecInitMPI(comm, Nlocal, Ny, \&argc, \&argv)}
followed by \newline
{\tt if (machEnv == NULL) return(1)}, to initialize the 
{\tt NVECTOR} module.  Here {\tt comm} is the MPI communicator, which
may be set by suitable MPI calls, for a proper subset of the active
processors, or else set to either {\tt NULL} or 
{\tt MPI\_COMM\_WORLD} to specify that all processors are to be used.

\item
The calling program must declare a real pointer (e.g., {\tt pbar}) and
set an array of real values {\tt pbar[i]} that are used to scale the
{\tt Ns} sensitivity vectors $w_i$.
Each {\tt pbar[i]} must be set to a nonzero constant that is
dimensionally consistent with {\tt p[i]}.
Typically, {\tt pbar[i]}$=${\tt p[i]} whenever {\tt p[i]} is nonzero.

\item
The pointer~{\tt plist} must be set to {\tt NULL} or point to an array
of integers.
For the latter, this array can be allocated using {\tt plist = (int *)
%malloc(NP * sizeof(int))}.
malloc(Ns * sizeof(int))}.
For the default setting {\tt plist = NULL}, {\tt yysub[i]} contains
the scaled sensitivity of $y$ with respect to $p_i$.
Otherwise, the user must set the integers in {\tt plist} to indicate
which of the {\tt Ns} sensitivity vectors are to be computed.
Namely, {\tt yysub[i]} will contain the scaled sensitivity of $y$ with
respect to $p_j$, where {\tt j=plist[i-1]} and {\tt i=1,...,Ns}.

\item Create and set {\tt N\_Vector yy} and {\tt N\_Vector yp} to initial
values for $Y$ and $\Yprime$, respectively.  
Depending on user options, also create the
vector {\tt id} of differential/algebraic component flags and/or the
vector {\tt constraints} of inequality constraint flags.
If an existing data array {\tt ydata} contains the initial values of
{\tt yy}, then call \newline
{\tt yy =} {\tt SensN\_VMAKE(Ntotal, ydata, machEnv)}.
Otherwise, make the call 
\newline
{\tt yy = N\_VNew(Ntotal, machEnv)} and load
initial values into the array of size {\tt Ntotal} defined by {\tt
N\_VDATA(yy)}. 
\newline
Conceptually, {\tt yy} consists of {\tt (1+Ns)} vectors of length {\tt
Ny}. 
If {\tt yy} was created with {\tt N\_VNew}, then load the {\tt Ny}
initial values with {\tt N\_VDATA(yy) = ydata} where {\tt ydata} is
a data array of size {\tt Ny}.
To load the $i$th sensitivity vector, use {\tt yysub =
N\_VSUB(yy); N\_VDATA(yysub[i]) = wdata}, where {\tt wdata} is an
existing data array of length {\tt Ny}.
Note that {\tt yysub[0]} is a pointer to the {\tt N\_Vector} of state
variables $y$, and that {\tt yysub[i]} is a pointer to the {\tt
N\_Vector} for the $i$th scaled sensitivity vector $w_i$.
Likewise, set {\tt yp} to $\Yprime$.

\item
If needed, several commands are available for identifying the
components of the {\tt Ns} sensitivity vectors as differential or
algebraic variables, for initializing the sensitivity vectors to zero,
and for setting the absolute error tolerances in the case of vector
tolerances.
To identify each component of the {\tt Ns} sensitivity vectors as a
differential or algebraic variable, use {\tt SensSetId(id, Ns)}, where
the vector {\tt id} is the differential/algebraic components flag used
for {\tt yy} in Step~7.
The {\tt Ns} sensitivity vectors can be initialized to zero using {\tt
SensInitZero(yy, Ns)}.
Finally, for the case of vector tolerances for absolute error control,
a typical default is to use the same {\tt atol} as for the state
variables $y$.  
Use {\tt SensSetVecAtol(atol, Ns)} to do this.  
Then, alter the resulting vector elements if desired.

\item 
{\tt ida\_mem = SensIDAMalloc(\ldots)} allocates internal
memory for IDA, initializes IDA, and returns a pointer to the
IDA memory structure. 
(See details below.)

\item Specify the linear system solver to be used by making one of the
calls: \newline 
{\tt flag = SensIDADense(...)} or {\tt flag = SensIDABand(...)} or
\newline
{\tt flag = SensIDASpgmr(...)}
followed by a test {\tt if (flag != 0) return(1)}.

\item Optionally, correct the initial values in {\tt yy, yp} with 
the call \newline
{\tt flag = IDACalcIC(idamem, icopt, ...)}; {\tt if (flag != 0) return(1)}.

\item 
{\tt iflag = IDASolve(ida\_mem, tout, yy, \&t, itask)} for each point
{\tt t =} {\tt tout} at which output is desired.
Set {\tt itask} to {\tt NORMAL} to have the integrator overshoot {\tt
tout} and interpolate, or {\tt ONE\_STEP} to take a single step and
return.
Alternatively, set {\tt itask = ONE\_STEP} to take one step forward
and return.
Also, test for the condition {\tt flag < 0} to detect a failure.
Following the call, process the vectors {\tt yy} and 
{\tt yp}, the computed solution $Y$ and $\Yprime$ at $t = $ {\tt tt}.
The {\em unscaled} sensitivity vector {\tt s\_i} is obtained by multiplying
{\tt yysub[i]} by the reciprocal of {\tt pbar[i]}.
To do this, call {\tt N\_VScale(1.0/pbar[i], yysub[i], s\_i)}.

\item 
{\tt SensIDAFree(ida\_mem)} to free the memory allocated for IDA.

\item 
The memory that was created for the vector {\tt yy} in Step~7 must be
deallocated: call {\tt N\_VFree(yy)} if {\tt yy} was allocated by {\tt
yy = N\_VNew(\ldots)}, or the user must call {\tt
SensN\_VDISPOSE(Ntotal, yy)} if {\tt yy} was allocated by {\tt yy =
SensN\_VMAKE(\ldots)}.

\item
Before freeing the pointer to the user-defined data block {\tt
rdata}, release the arrays containing the scale factors {\tt pbar}
and the real parameters {\tt p}:
{\tt free(pbar); free(rdata->p); free(rdata)}.

\item 
{\tt PVecFreeMPI(machEnv)} to free machine-dependent data.

\item
{\tt MPI\_Finalize();}
\end{enumerate}

\vspace{.1 in}

As indicated above, error conditions are possible at many of the
steps, and are flagged by nonzero return values.  In addition, error
messages are issued in most cases.

The form of the call to {\tt SensIDAMalloc} is 
\begin{verbatim}
      ida_mem = SensIDAMalloc(Ny, Ns, Ntotal, res, rdata, t0, y0, yp0, 
                    itol, rtol, atol, id, constraints, errfp, optIn, 
                    iopt[], ropt[], machEnv, p[], pbar[], plist, rhomax); 
\end{verbatim}
Except for a few additions, the arguments in {\tt SensIDAMalloc} are
the same as for the IDA routine {\tt IDAMalloc}:
the integer variables ({\tt Ny},
{\tt Ns}, {\tt Ntotal}), the real pointers ({\tt p}, {\tt pbar}), and
the integer pointer {\tt plist}
are described above; and the real variable {\tt rhomax} is the
finite difference threshold parameter ($\rhomax$); see the
description relating~(\ref{allterms}) to (\ref{switching}) in~$\S2$.
{\tt res} is the C function to compute $F(t,y,\yprime,p)$,
{\tt rdata} is a pointer to the user-defined data block passed
directly to the user's {\tt res} function,
{\tt t0} is the initial value of $t$,
{\tt y0} is the vector of length {\tt Ntotal} containing the initial values
of $Y$ (which can be the same as the vector {\tt yy} described above),
and {\tt yp0} is the vector of length {\tt Ntotal} containing the
initial value for the derivative $\Yprime$.
The next three parameters are used to set the error control.
The flag {\tt itol} is replaced by either {\tt SS} or {\tt SV}, where
{\tt SS} indicates scalar relative error tolerance and scalar absolute
error tolerance, while {\tt SV} indicates scalar relative error
tolerance and vector absolute error tolerance.
The latter choice is important when the absolute error tolerance needs
to be different for each component of the DAE.
The arguments {\tt \&rtol} and {\tt atol} are pointers to the user's
error tolerances.
{\tt id} is an {\tt N\_Vector}, required conditionally, which states a
given element to be either algebraic or differential.
{\tt constraints} is an {\tt N\_Vector} defining inequality
constraints for each component of the vector $Y$.
The file pointer {\tt errfp }points to the file where error messages
from SensIDA are to be written ({\tt NULL} for {\tt stdout}).
If {\tt optIn} is replaced by {\tt FALSE}, then the user is not going
to provide optional input, while if it is {\tt TRUE} then optional
inputs are examined in {\tt iopt} and {\tt ropt}.
{\tt iopt} and {\tt ropt} are integer and real arrays for optional
input and output.
{\tt machEnv} is a pointer to machine environment-specific
information.
Full details for the arguments common to {\tt IDAMalloc} can be found
in~\cite{IDA_UG}.

\subsection{User-Supplied Functions}

The user-supplied routines consist of one function defining the DAE residual, and
(optionally) one or two functions that define the preconditioner for
use in the SPGMR algorithm. 
All of the specifications are the same as when using IDA, as
documented in~\cite{IDA_UG}.
However, recall that {\tt Ny} refers to the number of DAEs contained
in {\tt res}, and the user-supplied data structure {\tt rdata}
contains a pointer (for example, {\tt p}) that points to the array of
real parameters used in {\tt res}.

The function $F(t,y,\yprime,p)$ defining the DAE system must be
supplied by the user in the form of a C function, denoted {\tt res} in
the Overview of Usage. 
\begin{verbatim}
     int res(integer Ny, real tres, N_Vector yy, N_Vector yp, 
             N_Vector resval, void *rdata)
\end{verbatim}
This function takes as input the problem size {\tt Ny}, the independent
variable value {\tt tres}, the dependent variable vector {\tt yy}, and
the derivative (with respect to t) of the {\tt yy} vector, {\tt yp}.
The computed value of $F(t, y, \yprime, p)$ is stored in {\tt resval}.

If preconditioning is used, then the user must provide a C function to
solve the linear system $Pz = r$ where $P$ is a left preconditioner
matrix.
Preconditioning is an important part of using IDA with the SPGMR
solver (or any Krylov solver). 
In any nontrivial DAE problem, it is usually essential to provide a
preconditioner of some sort.
This is primarily because the Krylov iteration convergence test is
based on the preconditioned residual vector.
Without preconditioning, this test can be a very poor measure of
convergence.

In supplying a preconditioner, the user must supply a C routine {\tt
PSolve} of the following form:
\begin{verbatim}
     int PSolve(integer Ny, real tt, N_Vector yy, N_Vector yp,
            N_Vector rr, real cj, ResFn res, void *rdata, void *pdata,
            N_Vector ewt, real delta, N_Vector rvec, N_Vector zvec,
            long int *nrePtr, N_Vector tempv)
\end{verbatim}
Its input includes 
{\tt tt}, the current value of the independent variable; 
{\tt yy}, the current value of the dependent variable vector $y$ of
length {\tt Ny}; 
{\tt yp}, the current value of the derivative vector $\yprime$;
{\tt rr}, the current vector of the residual vector
$F(t,y,\yprime,p)$; 
{\tt cj}, the scalar in the system Jacobian, proportional to $1/h$;
{\tt res}, the residual function for the DAE problem;
{\tt rdata}, a pointer to user data to be passed to {\tt res};
{\tt pdata}, a pointer to user preconditioner data.
Further input parameters are 
{\tt ewt}, the input error weight vector; 
{\tt delta}, an input tolerance if {\tt Psolve} is to use an iterative
method; 
{\tt rvec}, the input right-hand side vector {\tt r} in the linear system;
{\tt zvec}, the computed solution vector {\tt z};
{\tt nrePtr}, a pointer to the memory location containing the IDA
problem data {\tt nre} (the number of calls to {\tt res});
{\tt tempv}, a pointer to a memory location for temporary storage.
The integer returned value is to be negative if the {\tt Psolve}
function failed with an nonrecoverable error, 0 if {\tt Psolve} was
successful, or positive if there was a recoverable error.

If the user's preconditioner requires that any Jacobian related data
be evaluated or preprocessed, then this needs to be done in the
optional user-supplied C function {\tt Precond}.
The {\tt Precond} function has the form:
\begin{verbatim}
     int Precond(integer Ny, real tt, N_Vector yy, N_Vector yp,
           N_Vector rr, real cj, ResFn res, void *rdata, void *pdata,
           N_Vector ewt, N_Vector constraints, real hh, real uround,
           long int *nrePtr, N_Vector tempv1, N_Vector tempv2,
           N_Vector tempv3)
\end{verbatim}
The arguments which have not been discussed previously are the
following:
{\tt constraints}, the constraints vector;
{\tt hh}, a tentative step size in {\tt t};
{\tt uround}, is the machine unit roundoff;
{\tt tempv1}, {\tt tempv2}, {\tt tempv3} are temporary {\tt
N\_vector}s available for workspace.
The current stepsize {\tt hh} and unit roundoff {\tt uround} are
supplied for possible use in difference quotient calculations.

\subsection{Band-Block-Diagonal Preconditioner Module}
SensIDA has the same {\tt IBBDPRE} preconditioner module that is
included in IDA.
This preconditioner was developed to treat a rather broad class of
problems based on solving partial differential equations (PDEs) using
a method of lines approach.
The modules generate a preconditioner that is a block-diagonal
matrix, and each block contains a band matrix.
The blocks need not have the same number of super- and sub-diagonals;
these numbers may vary from block to block.
The IDA user's guide~\cite{IDA_UG} gives a complete description of
this preconditioning technique.
The basic idea is to isolate the preconditioning so that it is local
to each processor, and also to use a (possibly cheaper) approximate
residual function.
This requires the definition of a new function $G(t,y,\yprime,p)$
which approximates the function $F(t,y,\yprime,p)$ as given
in~(\ref{orig_DAE}). 
The choice $G=F$ is certainly allowed, but a less expensive choice may
be just as effective for preconditioning.

To use this {\tt IBBDPRE} module with SensIDA, the user must supply two
functions which the module calls to construct the preconditioner matrix~$P$.
These are in addition to the user-supplied residual function~{\tt res}.
\begin{itemize}
\item A function {\tt glocal(tt, yy, yp, gg, rdata)} must
be supplied by the user to compute~$G(t,y,\yprime,p)$.
It loads the vector {\tt gg} as a function of {\tt tt}, {\tt
yy}, {\tt yp} and the parameters~$p$ contained in~{\tt rdata}.
Although {\tt yy}, {\tt yp}, and {\tt gg} are all of type {\tt
N\_Vector}, only the local segment of each, of length {\tt Nlocal}, is
to be accessed in the routine {\tt glocal}.
\item A function {\tt gcomm(yy, yp, rdata)} which must be
supplied to perform all inter-processor communications necessary for
the execution of the {\tt glocal} function.
\end{itemize}

Both functions take as input the same pointer {\tt rdata} as that
passed by the user to {\tt SensIDAMalloc} and passed to the user's
function {\tt res}.
Both are to return an {\tt int} equal to 0 (indicating success), or
else 1 or -1 (indicating recoverable or non-recoverable failure,
respectively), just as for {\tt res}.
The user is responsible for providing space (presumably within {\tt
*rdata}) for data that are communicated by {\tt gcomm} from the other
processors, and that are then used by {\tt glocal}.
The function~{\tt glocal} is not expected to do any communication.

The usage of the {\tt IBBDPRE} module requires: (a) a call to {\tt
IBBDAlloc} to supply required parameters; and (b) passing specific
names for the preconditioning routines in the call to {\tt
SensIDASpgmr}.
See~\cite{IDA_UG} for details.

\subsection{Use by a C++ Application}

SensIDA is written in a manner that permits it to be used by
applications written in C++ as well as in C.  
For this purpose, each SensIDA header file is wrapped with
conditionally compiled lines reading {\tt extern "C" \{\ldots\}},
conditional on the variable {\tt \_\_cplusplus} being defined.
This directive causes the C++ compiler to use C-style names when
compiling the function prototypes encountered.
Users with C++ applications should also be aware that we have defined,
in {\tt llnltyps.h}, a boolean variable type, {\tt boole}, since C has
no such type.
The type {\tt boole} is equated to type {\tt int}, and so arguments in
user calls, or calls to user-supplied routines, which are of type {\tt
boole} can be typed as either {\tt boole} or {\tt int} by the user.
The same applies to vector kernels which have a type {\tt boole}
return value, if the user is providing these kernels.
The name {\tt boole} was chosen to avoid a conflict with the C++ type
{\tt bool}.

\section{Example Problems}

The SensIDA package includes eight sensitivity analysis example programs. 
These are based on three DAE system problems, two of which are solved in
several different ways.
The last of those two, a food web problem, is the most difficult and
most realistic.
Collectively, the examples are intended to illustrate the usage of
both the serial and parallel versions of SensIDA, the usage of all
three linear system modules, the use of the IDACalc routine for
obtaining consistent initial conditions, and the use of the IDABBDPRE
preconditioner module. In the following, we present the three DAE
problems and describe how each is solved with SensIDA.

\subsection{Robertson Kinetics Problem}

This example, due to Robertson, is a model of a three-species chemical
kinetics system written in DAE form. 
Differential equations are given for species $y^1$ and $y^2$ while an
algebraic equation determines $y^3$.
The equations for the system concentrations $y^{i}(t)$ are:
\begin{equation}
\left\{ \begin{array}{l}
               dy^1/dt = -p_1 y^1 + p_2 y^2 y^3   \\
               dy^2/dt = p_1 y^1 - p_2 y^2 y^3 - p_3 (y^2)^2  \\
               0 =  y^1 + y^2 + y^3 - 1
\end{array} \right.                                       \label{robsys}
\end{equation}

The initial values are: $y^1=1$, $y^2=0$, and $y^3=0$.
The parameter values are: $p_1 = 0.04$, $p_2 = 10^4$, and $p_3 = 3
\times 10^7$.
This example computes the three concentration components on the
interval from $t=0$ through $4 \times 10^{10}$.

This problem was solved only in one serial case using the simplest
linear solver module, IDADENSE.
It illustrates the application of IDADENSE, with a user-supplied
Jacobian function, for those problems to which a dense solver is
applicable.

The code and corresponding output can be found as {\tt sensrobx.c} and
{\tt sensrobx.output} in the distributed package.

\subsection{Heat Equation Problem}

This example solves a discretized 2D heat PDE problem. The DAE system
arises from the Dirichlet boundary condition $u = 0$, along with the 
differential equations arising from the discretization of the interior 
of the region. 

The equations solved are:
\begin{equation}
\left\{ \begin{array}{ll}
     \partial u / \partial t  = p_1 u_{xx} + p_2 u_{yy}  & \mbox{(interior)} \\
      u = 0. & \mbox{(boundary)}.
\end{array} \right.                                       \label{heateqsys}
\end{equation}
Initial conditions are given by $u = 16x(1-x)y(1-y)$, where the spatial domain
is the unit square $0 \leq x,y \leq 1$.
The parameter values are $p_1 = 1.0$, $p_2 = 1.0$, and the time
interval is $0 \leq t \leq 10.24$.

We discretize this PDE system (\ref{heateqsys}) (plus boundary conditions)
with central differencing on a $10 \times 10$ mesh, so as to obtain a
DAE system of size $N = 100$.  The dependent variable vector $u$
consists of the values $u^i(x_j,y_k,t)$ grouped first by $x$, and then
 by $y$.  At each spatial boundary point, the boundary condition is coupled
algebraically into the adjacent interior points by the central differencing
scheme. 

We solved this problem in four different ways, with the following
example programs:
\begin{enumerate}
\item {\tt sensheatsb:} serial version of SensIDA, band linear solver.
The half-bandwidths are 10. 
\item {\tt sensheatsk:} serial version of SensIDA, Krylov (GMRES)
linear solver with a user-supplied preconditioner.
As a preconditioner, we use the diagonal elements of the matrix $J$.
\item {\tt sensheatpk:} parallel version of SensIDA, Krylov (SPGMR)
linear solver with a user-supplied preconditioner.
We use a $5 \times 5$ subgrid on each of four processors.
For the preconditioner, we again use the diagonal elements of the
matrix $J$.
\item {\tt sensheatbbd:} parallel version of SensIDA, Krylov (SPGMR)
linear solver with {\tt IDABBDPRE} preconditioner module.
We use a $5 \times 5$ subgrid on each of 4 processors.
We use half-bandwidths {\tt mudq = mldq = 5} on each processor for the
difference quotient scheme, but keep only a tridiagonal matrix ({\tt
mu = ml = 1}).
\end{enumerate}

The source program for all four cases, along with the corresponding
output files are available in the distributed package.
They are not included in this document.

In the Appendix, we give the source and output of the {\tt sensheatpk}
program.

\subsection{Food Web Problem}

This example is a model of a multi-species food web \cite{Br86}, in
which predator-prey relationships with diffusion in a 2D spatial
domain are simulated.  Here we consider a model with $s = 2p$ species:
$p$ predators and $p$ prey.  Species $1,\ldots, p$ (the prey) satisfy
rate equations, while species $p+1,\ldots, s$ (the predators) have
infinitely fast reaction rates.  The coupled PDEs for the species
concentrations $c^i(x,y,t)$ are:
\begin{equation}
\left\{ \begin{array}{l}
               \partial c^i / \partial t = R_i(x,y,c) + d_i 
( c^i_{xx} + c^i_{yy} ) ~ ~ (i = 1,2,\ldots,p) ~, \\
                \hspace*{.36in}        0 = R_i(x,y,c) + d_i 
( c^i_{xx} + c^i_{yy} ) ~ ~ (i = p+1,\ldots,s) ~,
\end{array} \right.                                       \label{ppsys}
\end{equation}
with
\[
R_i(x,y,c) = c^i (b_i + \sum_{j=1}^s a_{ij} c^j) ~.
\]
Here $c$ denotes the vector $\{c^i\}$.
The interaction and diffusion coefficients $(a_{ij},b_i,d_i)$ can be
functions of $(x,y)$ in general. The choices made for this test
problem are as follows, and include 2 parameters in the term $b_i$:
\[
\left\{ \begin{array}{l}
a_{ii} = -1 ~ ~ (\mbox{all } i) \\
a_{ij} = -0.5 \cdot 10^{-6} ~ ~ ( i \leq p , j > p ) \\
a_{ij} = 10^4 ~ ( i > p , j \leq p ) \\
\mbox{(all other } a_{ij} = 0 ) ~,
\end{array} \right.
\]

\[
\left\{ \begin{array}{l}
b_i = b_i(x,y) =  (1 + p_1 xy + p_2 \sin(4\pi x)\sin(4\pi y) )
                   ~~ ( i \leq p ) \\
b_i = b_i(x,y) = -(1 + p_1 xy + p_2 \sin(4\pi x)\sin(4\pi y) )
                   ~~ ( i > p ) ~,
\end{array} \right.
\]
and
\[
\left\{ \begin{array}{l}
d_i =   1 ~ ~ ( i \leq p ) \\
d_i = 0.5 ~ ~ ( i > p ) .
\end{array} \right.
\]

The spatial domain is the unit square $0 \leq x,y \leq 1$, and the
time interval is $0 \leq t \leq 1$.  
The parameters values are: $p_1 = 50$, $p_2 = 1000$.
The boundary conditions are of
Neumann type (zero normal derivatives) everywhere.  The coefficients
are such that a unique stable equilibrium is guaranteed to exist when
$p_1 = p_2 = 0$.  Empirically, a stable equilibrium
appears to exist for (\ref{ppsys}) when $p_1$ and $p_2$ are
positive, although it may not be unique. 
For the initial conditions, we set each prey concentration to a simple
polynomial profile satisfying the boundary conditions, while the
predator concentrations are all set to a flat value:
\[
\left\{ \begin{array}{l}
c^i(x,y,0) = 10 + i [16x(1 - x)y(1 - y)]^2 ~~ (i \leq p) ~, \\
c^i(x,y,0) = 10^5 ~~ (i > p) ~.
\end{array} \right.
\]

We discretize this PDE system (\ref{ppsys}) (plus boundary conditions)
with central differencing on an $L \times L$ mesh, so as to obtain a
DAE system of size $N = s L^2$.  The dependent variable vector $C$
consists of the values $c^i(x_j,y_k,t)$ grouped first by species index
$i$, then by $x$, and lastly by $y$.  At each spatial mesh point, the
system has a block of $p$ ODE's followed by a block of $p$ algebraic
equations, all coupled.
For this example, we take $p = 1, s = 2$, and $L = 20$.  See also 
\cite{BHP98}, where various cases of this problem are solved with DASPK.

This problem was solved in three different ways, with the following
three example programs:
\begin{enumerate}
\item {\tt senswebsb:} serial version of SensIDA, band linear solver.
The half-bandwidths are {\tt mu = ml = sL = 40}.
\item {\tt senswebpk:} parallel version of SensIDA, Krylov (SPGMR)
linear solver with a user-supplied preconditioner.
We use a $Lsub \times Lsub$ subgrid, with $Lsub = 10$, on each of four
processors.
For the preconditioner, we take the block-diagonal matrix with $2
\times 2$ blocks arising from the reaction coefficients $\partial R_i
/ \partial c$ only.
\item {\tt senswebbbd:} parallel version of SensIDA, Krylov (SPGMR)
linear solver with {\tt IDABBDPRE} preconditioner module.
We use half-bandwidths {\tt mudq = mldq} $= s Lsub = 20$ for the
difference quotient scheme, but retain only a matrix with bandwidth
$5$ by setting {\tt mu = ml = 2}.
\end{enumerate}

In all three cases, the flat predator initial values are not
consistent with the quasi-steady equations for the predator species,
and so we call {\tt IDACalcIC} to correct those values. 
In the two parallel programs, we use a logically square array of
processors and corresponding Cartesian subdomain decomposition.

\section{Availability}

The SensIDA package is being released for general distribution at
this time.
Interested users should contact Steven Lee (slee@llnl.gov ) or Alan
Hindmarsh (alanh@llnl.gov).

\begin{thebibliography}{99}

\bibitem{Br86}
Peter N. Brown, {\em Decay to Uniform States in Food Webs}, SIAM
J. Appl. Math., 46 (1986), pp. 376--392.

\bibitem{BH89} Peter N. Brown and Alan C. Hindmarsh, {\em Reduced Storage
Matrix Methods in Stiff ODE Systems}, J. Appl. Math. \& Comp., {\bf 31}
(1989), pp. 40-91.

\bibitem{DASPK2}
Peter N. Brown, Alan C. Hindmarsh, and Linda R. Petzold, {\em Using Krylov
Methods in the Solution of Large-Scale Differential-Algebraic Systems}, 
SIAM J. Sci. Comput., 15 (1994), pp. 1467--1488.

\bibitem{BHP98}
Peter N. Brown, Alan C. Hindmarsh, and Linda R. Petzold, {\em Consistent
Initial Condition Calculation for Differential-Algebraic Systems}, 
SIAM J. Sci. Comput., 19 (1998), pp. 1495--1512.

\bibitem{PVODE_UG} George D. Byrne and Alan C. Hindmarsh, {\it User
Documentation for PVODE, an ODE Solver for Parallel Computers},
Lawrence Livermore National Laboratory report UCRL-ID-130884, May 1998.
See also the Addenda in the doc subdirectory of the current version of
{\it PVODE}.

\bibitem{CVODE_UG} Scott D. Cohen and Alan C. Hindmarsh, {\it CVODE User
Guide}, Lawrence Livermore National Laboratory report UCRL-MA-118618, Sept.
1994.

\bibitem{CVODE} Scott D. Cohen and Alan C. Hindmarsh, {\em CVODE, a
Stiff/Nonstiff ODE Solver in C}, Computers in Physics, {\bf 10}, No. 2
(1996), pp. 138--143.

\bibitem{MPI}  William Gropp, Ewing Lusk, and Anthony Skjellum, {\it Using
MPI Portable Parallel Programming with the Message-Passing Interface}, The
MIT Press, Cambridge, MA, 1994.

\bibitem{IDA_UG} Alan C. Hindmarsh and Allan G. Taylor, {\it User
Documentation for IDA, a Differential-Algebraic Equation Solver for
Sequential and Parallel Computers},
Lawrence Livermore National Laboratory report UCRL-MA-136910, December 1999.

\bibitem{SensPVODE_UG} Steven L. Lee, Alan C. Hindmarsh, and Peter N. Brown,
{\it User Documentation for SensPVODE, a Variant of PVODE for
Sensitivity Analysis}, Lawrence Livermore National Laboratory report
UCRL-MA-140211, August 2000.

\bibitem{DASPK3}
Shengtai Li and Linda R. Petzold, {\em Software and Algorithms for Sensitivity
Analysis of Large-Scale Differential Algebraic Systems}, To appear
J. Comp. Appl. Math.

\bibitem{MP96}  Timothy Maly and Linda R. Petzold, {\em Numerical Methods and
Software for Sensitivity Analysis of Differential-algebraic systems},
Appl. Numer. Math., {\bf 20} (1996), pp. 57--79.

\bibitem{GMRES}  Yousef Saad and Martin Schultz, {\em GMRES: A Generalized
Minimal Residual Algorithm for Solving Nonsymmetric Linear Systems},
SIAM J. Sci. Stat. Comput., {\bf 11} (1990), 856-869.

\end{thebibliography}

\newpage
\section{Appendix: Listing of Heat Equation Example, with Sensitivity
Analysis}
%Predator-Prey Example
\begin{verbatim}
/***********************************************************************
 * File:       sensheatpk.c   
 * Written by: Steven L. Lee and Alan C. Hindmarsh
 *----------------------------------------------------------------------
 *
 * Sensitivity analysis version of Example problem for SensIDA:
 * 2D heat equation, parallel, GMRES.
 *
 * This example solves a discretized 2D heat equation problem.
 *
 *
 * The DAE system solved is a spatial discretization of the PDE 
 *
 *          du/dt = p1*d^2u/dx^2 + p2*d^2u/dy^2
 *
 * on the unit square, where p1 = 1.0 and p2 = 1.0.  
 * The boundary condition is u = 0 on all edges.
 * Initial conditions are given by u = 16 x (1 - x) y (1 - y).
 * The PDE is treated with central differences on a uniform MX x MY grid.
 * The values of u at the interior points satisfy ODEs, and equations
 * u = 0 at the boundaries are appended, to form a DAE system of size
 * N = MX * MY.  Here MX = MY = 10.
 *
 * The system is actually implemented on submeshes, processor by processor,
 * with an MXSUB by MYSUB mesh on each of NPEX * NPEY processors.
 *
 * The system is solved with SensIDA using the Krylov linear solver IDASPGMR. 
 * The preconditioner uses the diagonal elements of the Jacobian only.
 * Routines for preconditioning, required by IDASPGMR, are supplied here.
 * The constraints u >= 0 are posed for all components.
 * Local error testing on the boundary values is suppressed. 
 * Output is taken at t = 0, .01, .02, .04, ..., 10.24.
 *
 ***********************************************************************/

#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include "llnltyps.h"
#include "llnlmath.h"
#include "nvector.h"
#include "ida.h"
#include "idaspgmr.h"
#include "iterativ.h"
#include "mpi.h"
#include "sensida.h"

#define ZERO  RCONST(0.0)
#define ONE   RCONST(1.0)
#define TWO   RCONST(2.0)


#define NOUT         11             /* Number of output times */

#define NPEX         2              /* No. PEs in x direction of PE array */
#define NPEY         2              /* No. PEs in y direction of PE array */
                                    /* Total no. PEs = NPEX*NPEY */
#define MXSUB        5              /* No. x points per subgrid */
#define MYSUB        5              /* No. y points per subgrid */

#define MX           (NPEX*MXSUB)   /* MX = number of x mesh points */
#define MY           (NPEY*MYSUB)   /* MY = number of y mesh points */
                                    /* Spatial mesh is MX by MY */
#define NY           (MX*MY)        /* number of equations      */
#define NP           2              /* number of parameters     */
#define NS           2              /* number of sensitivities  */

typedef struct {  
  real      *p;
  integer   neq, thispe, mx, my, ixsub, jysub, npex, npey, mxsub, mysub;
  real      dx, dy, coeffx, coeffy, coeffxy;
  real      uext[(MXSUB+2)*(MYSUB+2)];
  N_Vector  pp;    /* vector of diagonal preconditioner elements */
  MPI_Comm  comm;
} *UserData;


/* Prototypes of private helper functions */

static int InitUserData(integer Neq, integer thispe, integer npes,
			MPI_Comm comm, UserData data);

static int SetInitialProfile(N_Vector uu, N_Vector up, N_Vector id,
			     N_Vector res, UserData data);


/* User-supplied residual function and supporting routines */


int heatres(integer Neq, real tres, N_Vector uu, N_Vector up,
            N_Vector res, void *rdata);

static int rescomm(N_Vector uu, N_Vector up, void *rdata);

static int reslocal(real tres, N_Vector uu, N_Vector up, 
		    N_Vector res,  void *rdata);

static int BSend(MPI_Comm comm, integer thispe, integer ixsub, integer jysub,
                 integer dsizex, integer dsizey, real uarray[]);

static int BRecvPost(MPI_Comm comm, MPI_Request request[], integer thispe,
		     integer ixsub, integer jysub,
		     integer dsizex, integer dsizey,
		     real uext[], real buffer[]);

static int BRecvWait(MPI_Request request[], integer ixsub, integer jysub,
		     integer dsizex, real uext[], real buffer[]);

/* User-supplied preconditioner routines */

int PSolveHeateq(integer local_N, real tt, N_Vector uu,
                 N_Vector up, N_Vector rr, real cj, ResFn res, void *rdata,
                 void *pdata, N_Vector ewt, real delta, N_Vector rvec,
                 N_Vector zvec, long int *nrePtr, N_Vector tempv);


int PrecondHeateq(integer local_N, real tt, N_Vector yy,
                  N_Vector yp, N_Vector rr, real cj,
                  ResFn res, void *rdata, void *pdata,
                  N_Vector ewt, N_Vector constraints, real hh, 
                  real uround, long int *nrePtr,
                  N_Vector tempv1, N_Vector tempv2, N_Vector tempv3);


main(int argc, char *argv[])

{
  integer retval, i, j, iout, itol, itask, local_N, npes, thispe;
  long int iopt[OPT_SIZE];
  boole optIn;
  real ropt[OPT_SIZE], rtol, atol;
  real t0, t1, tout, tret, umax;
  void *mem;
  UserData data;
  N_Vector uu, up, constraints, id, res;
  IDAMem idamem;
  MPI_Comm comm;
  machEnvType machEnv;
  N_Vector *uusub, *upsub;
  N_Vector *constraintssub;
  integer Ntotal;
  real *pbar;
  real rhomax;
  int *plist;

  Ntotal = (1+NS)*NY;

  /* Get processor number and total number of pe's. */
  MPI_Init(&argc, &argv);
  comm = MPI_COMM_WORLD;
  MPI_Comm_size(comm, &npes);
  MPI_Comm_rank(comm, &thispe);

  if (npes != NPEX*NPEY) {
    if (thispe == 0)
      printf("\n npes=%d is not equal to NPEX*NPEY=%d\n", npes,NPEX*NPEY);
    return(1);
  }

  /* Set local length local_N. */
  local_N = MXSUB*MYSUB;

  /* Set machEnv block. */
  machEnv = PVecInitMPI(comm, local_N, NY, &argc, &argv);
  if (machEnv == NULL) return(1);

  /* Allocate and initialize the data structure and N-vectors. */
  data = (UserData) malloc(sizeof *data);

  /* Store nominal parameter values in p */
  data->p = (real *) malloc(NP * sizeof(real));
  data->p[0] = 1.0;
  data->p[1] = 1.0;

  /* Scaling factor for each sensitivity equation */
  pbar = (real *) malloc(NP * sizeof(real));
  pbar[0] = 1.0;
  pbar[1] = 1.0;

  /* Store ordering of parameters in plist */
  plist = (int *) malloc(NP * sizeof(int));
  plist[0] = 1;
  plist[1] = 2;

  rhomax = 0.0;

  uu = N_VNew(Ntotal, machEnv); 
  up = N_VNew(Ntotal, machEnv);
  res = N_VNew(Ntotal, machEnv);
  constraints = N_VNew(Ntotal, machEnv);
  id = N_VNew(Ntotal, machEnv);

  /* Create pointers to subvectors */
  uusub = N_VSUB(uu);
  upsub = N_VSUB(up);
  constraintssub = N_VSUB(constraints);

  data->pp = N_VNew(NY, machEnv); /* An N-vector to hold preconditioner. */

  InitUserData(NY, thispe, npes, comm, data);

  /* Initialize the uu, up, id, and res profiles. */
  SetInitialProfile(uu, up, id, res, data);

  /* Set constraints to all 1's for nonnegative solution values in y. */
  N_VConst(ONE, constraintssub[0]);

  /* Initialize the sensitivity variables */
  SensInitZero(uu, NS);
  SensInitZero(up, NS);
  SensInitZero(constraints, NS);

  /* Identify all sensitivity variables as differential variables */
  SensSetId(id, NS);

  t0 = 0.0; t1 = 0.01;

  /* Scalar relative and absolute tolerance. */
  itol = SS;
  rtol = 0.0;
  atol = 1.e-3;

  /* Set option to suppress error testing of algebraic components. */
  optIn = TRUE;
  for (i = 0; i < OPT_SIZE; i++) {iopt[i] = 0; ropt[i] = ZERO; }
  iopt[SUPPRESSALG] = 1;

  /* Call SensIDAMalloc to initialize solution.  (NULL argument is errfp.) */
  itask = NORMAL;

  mem = SensIDAMalloc(NY, NS, Ntotal, heatres, data, t0, uu, up, 
		      itol, &rtol, &atol, id, constraints, NULL, optIn, 
		      iopt, ropt, machEnv, data->p, pbar, plist, rhomax);
  if (mem == NULL) {
    if (thispe == 0) printf ("SensIDAMalloc failed.");
    return(1); }
  idamem = (IDAMem)mem;

  /* Call SensIDASpgmr to specify the linear solver. */
  retval = SensIDASpgmr(idamem, PrecondHeateq, PSolveHeateq,  MODIFIED_GS, 
                    0, 0, 0.0, 0.0, data);

  if (retval != SUCCESS) {
    if (thispe == 0) printf("SensIDASpgmr failed, returning %d.\n",retval);
    return(1);
  }

  /* Call IDACalcIC (with default options) to correct the initial values. */
  retval = IDACalcIC(idamem, CALC_YA_YDP_INIT, t1, ZERO, 0,0,0,0, ZERO);

  if (retval != SUCCESS) {
    if (thispe == 0) printf("IDACalcIC failed. retval = %d\n", retval);
    return(1);
  }

  /* Compute the max norm of uu. */
  umax = N_VMaxNorm(uusub[0]);

  /* Print output heading (on processor 0 only). */

  if (thispe == 0) { 
    printf("sensheatpk: Heat equation, parallel example problem for SensIDA \n");
    printf("            Discretized heat equation on 2D unit square. \n");
    printf("            Zero boundary conditions,");
    printf(" polynomial initial conditions.\n");
    printf("            Mesh dimensions: %d x %d", MX, MY);
    printf("            Total system size: %d\n\n", NY);
    printf("Subgrid dimensions: %d x %d", MXSUB, MYSUB);
    printf("            Processor array: %d x %d\n", NPEX, NPEY);
    printf("Number of sensitivities: Ns = %d\n", NS);
    printf("Parameter values:       p_1 = %9.2e,    p_2 = %9.2e\n", 
	   data->p[0], data->p[1]);
    printf("Scale factors:       pbar_1 = %9.2e, pbar_2 = %9.2e\n", 
	   pbar[0], pbar[1]);
    printf("Finite difference:   rhomax = %g\n", rhomax);
    printf("Tolerance parameters:  rtol = %g,  atol = %g\n", rtol, atol);
    printf("Constraints set to force all components of solution u >= 0. \n");
    printf("iopt[SUPPRESSALG] = 1 to suppress local error testing on");
    printf(" all boundary components. \n");
    printf("Linear solver: IDASPGMR  ");
    printf("Preconditioner: diagonal elements only.\n"); 

    /* Print output table heading and initial line of table. */
    printf("\n");
    printf("Output Summary:   max(u) = max-norm of solution u \n");
    printf("                max(s_i) = max-norm of sensitivity vector s_i\n\n");
    printf("  time       max(u)     k   nst  nni  nli   nre    h       npe nps\n");
    printf(" .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .\n");

    printf(" %5.2f   %13.5e  %d  %3d  %3d  %3d  %4d %9.2e  %3d %3d\n",
           t0, umax, iopt[KUSED], iopt[NST], iopt[NNI], iopt[SPGMR_NLI], 
           iopt[NRE], ropt[HUSED], iopt[SPGMR_NPE], iopt[SPGMR_NPS]);
  }

  for (i = 1; i <= NS; i++){
    umax = N_VMaxNorm(uusub[i]);
    j = (plist == NULL) ? i : plist[i-1];
    if (thispe == 0) {
      printf("max(s_%d) = %11.5e\n", j, umax/pbar[j-1]);
      if (i == NS) printf("\n");
    }
  }

  /* Loop over tout, call IDASolve, print output. */

  for (tout = t1, iout = 1; iout <= NOUT; iout++, tout *= TWO) {

    retval = IDASolve(idamem, tout, t0, &tret, uu, up, itask);

    umax = N_VMaxNorm(uusub[0]);
    if (thispe == 0) 
	printf(" %5.2f   %13.5e  %d  %3d  %3d  %3d  %4d %9.2e  %3d %3d\n",
 	       tret, umax,iopt[KUSED],iopt[NST],iopt[NNI],iopt[SPGMR_NLI], 
               iopt[NRE], ropt[HUSED], iopt[SPGMR_NPE], iopt[SPGMR_NPS]);

    for (i = 1; i <= NS; i++){
      umax = N_VMaxNorm(uusub[i]);
      j = (plist == NULL) ? i: plist[i-1];
      if (thispe == 0) {
	printf("max(s_%d) = %11.5e\n", j, umax/pbar[j-1]);
	if (i == NS) printf("\n");
      }
    }

    if (retval < 0) {
      if (thispe == 0) printf("IDASolve returned %d.\n",retval);
      return(1);
    }

  }  /* End of tout loop. */

  /* Print remaining counters and free memory. */
  if (thispe == 0) printf("\n netf = %d,   ncfn = %d,   ncfl = %d \n", 
                   iopt[NETF], iopt[NCFN], iopt[SPGMR_NCFL]);

  SensIDAFree(idamem);
  N_VFree(uu);
  N_VFree(up);
  N_VFree(constraints);
  N_VFree(id);
  N_VFree(res);
  N_VFree(data->pp);

  if (plist != NULL) free(plist);
  free(pbar);
  free(data->p);
  free(data);
  PVecFreeMPI(machEnv);
  MPI_Finalize();
  return(0);

} /* End of sensheatpk main program. */


/********************************************************/
/* InitUserData initializes the user's data block data. */

static int InitUserData(integer Neq, integer thispe, integer npes,
		        MPI_Comm comm, UserData data)
{

  data->neq = Neq;
  data->thispe = thispe;
  data->dx = ONE/(MX-ONE);       /* Assumes a [0,1] interval in x. */
  data->dy = ONE/(MY-ONE);       /* Assumes a [0,1] interval in y. */
  data->coeffx  = ONE/(data->dx * data->dx);
  data->coeffy  = ONE/(data->dy * data->dy);
  data->coeffxy = TWO/(data->dx * data->dx) + TWO/(data->dy * data->dy);
  data->jysub   = thispe/NPEX;
  data->ixsub   = thispe - data->jysub * NPEX;
  data->npex    = NPEX;
  data->npey    = NPEY;
  data->mx      = MX;
  data->my      = MY;
  data->mxsub = MXSUB;
  data->mysub = MYSUB;
  data->comm    = comm;
  return(0);

} /* End of InitUserData. */


/**************************************************************/
/* SetInitialProfile sets the initial values for the problem. */

static int SetInitialProfile(N_Vector uu, N_Vector up,  N_Vector id, 
			     N_Vector res, UserData data)
{
  integer i, iloc, j, jloc, offset, loc, ixsub, jysub;
  integer ixbegin, ixend, jybegin, jyend;
  real xfact, yfact, *udata, *iddata, dx, dy;

  /* Initialize uu. */ 

  udata = N_VDATA(uu);
  iddata = N_VDATA(id);

  /* Set mesh spacings and subgrid indices for this PE. */
  dx = data->dx;
  dy = data->dy;
  ixsub = data->ixsub;
  jysub = data->jysub;

  /* Set beginning and ending locations in the global array corresponding 
     to the portion of that array assigned to this processor. */
  ixbegin = MXSUB*ixsub;
  ixend   = MXSUB*(ixsub+1) - 1;
  jybegin = MYSUB*jysub;
  jyend   = MYSUB*(jysub+1) - 1;

 /* Loop over the local array, computing the initial profile value.
    The global indices are (i,j) and the local indices are (iloc,jloc).
    Also set the id vector to zero for boundary points, one otherwise. */

  N_VConst(ONE, id);
  for (j = jybegin, jloc = 0; j <= jyend; j++, jloc++) {
    yfact = data->dy*j;
    offset= jloc*MXSUB;
    for (i = ixbegin, iloc = 0; i <= ixend; i++, iloc++) {
      xfact = data->dx * i;
      loc = offset + iloc;
      udata[loc] = 16. * xfact * (ONE - xfact) * yfact * (ONE - yfact);
      if (i == 0 || i == MX-1 || j == 0 || j == MY-1) iddata[loc] = ZERO;
      }
    }

  /* Initialize up. */

  N_VConst(ZERO, up);    /* Initially set up = 0. */

  /* heatres sets res to negative of ODE RHS values at interior points. */
  heatres(data->neq, ZERO, uu, up, res, data);

  /* Copy -res into up to get correct initial up values. */
  N_VScale(-ONE, res, up);

  return(SUCCESS);

} /* End of SetInitialProfiles. */


/***************** Functions called by the IDA solver ******************/


/*************************************************************************
 * heatres: heat equation system residual function                       
 * This uses 5-point central differencing on the interior points, and    
 * includes algebraic equations for the boundary values.                 
 * So for each interior point, the residual component has the form       
 *    res_i = u'_i - (central difference)_i                              
 * while for each boundary point, it is res_i = u_i. 
 *                    
 * This parallel implementation uses several supporting routines. 
 * First a call is made to rescomm to do communication of subgrid boundary
 * data into array uext.  Then reslocal is called to compute the residual
 * on individual processors and their corresponding domains.  The routines
 * BSend, BRecvPost, and BREcvWait handle interprocessor communication
 * of uu required to calculate the residual. */

int heatres(integer Neq, real tres, N_Vector uu, N_Vector up,
              N_Vector res, void *rdata)
{
  int retval;
  UserData data;

  data = (UserData) rdata;

  /* Call rescomm to do inter-processor communication. */
  retval = rescomm(uu, up, data);

  /* Call reslocal to calculate res. */
  retval = reslocal(tres, uu, up, res, data);
  
  return(0);

} /* End of residual function heatres. */


/*  Supporting functions for heatres. */


/*********************************************************************/
/* rescomm routine.  This routine performs all inter-processor
   communication of data in u needed to calculate G.                 */

static int rescomm(N_Vector uu, N_Vector up, void *rdata)
{
  UserData data;
  real *uarray, *uext, buffer[2*MYSUB];
  MPI_Comm comm;
  integer thispe, ixsub, jysub, mxsub, mysub;
  MPI_Request request[4];

  data = (UserData) rdata;
  uarray = N_VDATA(uu);

  /* Get comm, thispe, subgrid indices, data sizes, extended array uext. */
  comm = data->comm;  thispe = data->thispe;
  ixsub = data->ixsub;   jysub = data->jysub;
  mxsub = data->mxsub;   mysub = data->mysub;
  uext = data->uext;

  /* Start receiving boundary data from neighboring PEs. */
  BRecvPost(comm, request, thispe, ixsub, jysub, mxsub, mysub, uext, buffer);

  /* Send data from boundary of local grid to neighboring PEs. */
  BSend(comm, thispe, ixsub, jysub, mxsub, mysub, uarray);

  /* Finish receiving boundary data from neighboring PEs. */
  BRecvWait(request, ixsub, jysub, mxsub, uext, buffer);

  return(0);

} /* End of rescomm. */


/**************************************************************************/
/* reslocal routine.  Compute res = F(t, uu, up).  This routine assumes
   that all inter-processor communication of data needed to calculate F
    has already been done, and that this data is in the work array uext.  */

static int reslocal(real tres, N_Vector uu, N_Vector up, N_Vector res,
                    void *rdata)
{
  real *uext, *uuv, *upv, *resv;
  real termx, termy, termctr;
  integer i, lx, j, ly, offsetu, offsetue, locu, locue;
  integer ixsub, jysub, mxsub, mxsub2, mysub, npex, npey;
  integer ixbegin, ixend, jybegin, jyend;
  UserData data;
  real p1, p2;

  /* Get subgrid indices, array sizes, extended work array uext. */

  data = (UserData) rdata;
  p1 = data->p[0];
  p2 = data->p[1];
  uext = data->uext;
  uuv = N_VDATA(uu);
  upv = N_VDATA(up);
  resv = N_VDATA(res);
  ixsub = data->ixsub; jysub = data->jysub;
  mxsub = data->mxsub; mxsub2 = data->mxsub + 2;
  mysub = data->mysub; npex = data->npex; npey = data->npey;

  /* Initialize all elements of res to uu. This sets the boundary
     elements simply without indexing hassles. */

  N_VScale(ONE, uu, res);

  /* Copy local segment of u vector into the working extended array uext.
     This completes uext prior to the computation of the res vector.     */

  offsetu = 0;
  offsetue = mxsub2 + 1;
  for (ly = 0; ly < mysub; ly++) {
    for (lx = 0; lx < mxsub; lx++) uext[offsetue+lx] = uuv[offsetu+lx];
    offsetu = offsetu + mxsub;
    offsetue = offsetue + mxsub2;
  }

  /* Set loop limits for the interior of the local subgrid. */

  ixbegin = 0;
  ixend   = mxsub-1;
  jybegin = 0;
  jyend   = mysub-1;
  if (ixsub == 0) ixbegin++; if (ixsub == npex-1) ixend--;
  if (jysub == 0) jybegin++; if (jysub == npey-1) jyend--;
  
  /* Loop over all grid points in local subgrid. */

  for (ly = jybegin; ly <=jyend; ly++) {
    for (lx = ixbegin; lx <= ixend; lx++) {
      locu  = lx + ly*mxsub;
      locue = (lx+1) + (ly+1)*mxsub2;
      termx = p1 * data->coeffx *(uext[locue-1]      + uext[locue+1]);
      termy = p2 * data->coeffy *(uext[locue-mxsub2] + uext[locue+mxsub2]);
      termctr = (p1*(data->coeffx) + p2*(data->coeffy)) * TWO * uext[locue];
      resv[locu] = upv[locu] - (termx + termy - termctr);
   }
  }
  return(0);

} /* End of reslocal. */


/*************************************************************************/
/* Routine to send boundary data to neighboring PEs.                     */

static int BSend(MPI_Comm comm, integer thispe, integer ixsub, integer jysub,
                 integer dsizex, integer dsizey, real uarray[])
{
  integer ly, offsetu;
  real bufleft[MYSUB], bufright[MYSUB];

  /* If jysub > 0, send data from bottom x-line of u. */

  if (jysub != 0)
    MPI_Send(&uarray[0], dsizex, PVEC_REAL_MPI_TYPE, thispe-NPEX, 0, comm);

  /* If jysub < NPEY-1, send data from top x-line of u. */

  if (jysub != NPEY-1) {
    offsetu = (MYSUB-1)*dsizex;
    MPI_Send(&uarray[offsetu], dsizex, PVEC_REAL_MPI_TYPE, 
           thispe+NPEX, 0, comm);
  }

  /* If ixsub > 0, send data from left y-line of u (via bufleft). */

  if (ixsub != 0) {
    for (ly = 0; ly < MYSUB; ly++) {
      offsetu = ly*dsizex;
      bufleft[ly] = uarray[offsetu];
    }
    MPI_Send(&bufleft[0], dsizey, PVEC_REAL_MPI_TYPE, thispe-1, 0, comm);   
  }

  /* If ixsub < NPEX-1, send data from right y-line of u (via bufright). */

  if (ixsub != NPEX-1) {
    for (ly = 0; ly < MYSUB; ly++) {
      offsetu = ly*MXSUB + (MXSUB-1);
      bufright[ly] = uarray[offsetu];
    }
    MPI_Send(&bufright[0], dsizey, PVEC_REAL_MPI_TYPE, thispe+1, 0, comm);   
  }
} /* End of BSend. */


/**************************************************************/
/* Routine to start receiving boundary data from neighboring PEs.
   Notes:
   1) buffer should be able to hold 2*MYSUB real entries, should be
   passed to both the BRecvPost and BRecvWait functions, and should not
   be manipulated between the two calls.
   2) request should have 4 entries, and should be passed in 
   both calls also. */

static int BRecvPost(MPI_Comm comm, MPI_Request request[], integer thispe,
		     integer ixsub, integer jysub,
		     integer dsizex, integer dsizey,
		     real uext[], real buffer[])
{
  integer offsetue;
  /* Have bufleft and bufright use the same buffer. */
  real *bufleft = buffer, *bufright = buffer+MYSUB;

  /* If jysub > 0, receive data for bottom x-line of uext. */
  if (jysub != 0)
    MPI_Irecv(&uext[1], dsizex, PVEC_REAL_MPI_TYPE,
    					 thispe-NPEX, 0, comm, &request[0]);

  /* If jysub < NPEY-1, receive data for top x-line of uext. */
  if (jysub != NPEY-1) {
    offsetue = (1 + (MYSUB+1)*(MXSUB+2));
    MPI_Irecv(&uext[offsetue], dsizex, PVEC_REAL_MPI_TYPE,
                                         thispe+NPEX, 0, comm, &request[1]);
  }

  /* If ixsub > 0, receive data for left y-line of uext (via bufleft). */
  if (ixsub != 0) {
    MPI_Irecv(&bufleft[0], dsizey, PVEC_REAL_MPI_TYPE,
                                         thispe-1, 0, comm, &request[2]);
  }

 /* If ixsub < NPEX-1, receive data for right y-line of uext (via bufright). */
  if (ixsub != NPEX-1) {
    MPI_Irecv(&bufright[0], dsizey, PVEC_REAL_MPI_TYPE,
                                         thispe+1, 0, comm, &request[3]);
  }

} /* End of BRecvPost. */


/****************************************************************/
/* Routine to finish receiving boundary data from neighboring PEs.
   Notes:
   1) buffer should be able to hold 2*MYSUB real entries, should be
   passed to both the BRecvPost and BRecvWait functions, and should not
   be manipulated between the two calls.
   2) request should have four entries, and should be passed in both 
      calls also. */

static int BRecvWait(MPI_Request request[], integer ixsub, integer jysub,
		      integer dsizex, real uext[], real buffer[])
{
  integer ly, dsizex2, offsetue;
  real *bufleft = buffer, *bufright = buffer+MYSUB;
  MPI_Status status;

  dsizex2 = dsizex + 2;

  /* If jysub > 0, receive data for bottom x-line of uext. */
  if (jysub != 0)
    MPI_Wait(&request[0],&status);

  /* If jysub < NPEY-1, receive data for top x-line of uext. */
  if (jysub != NPEY-1)
    MPI_Wait(&request[1],&status);

  /* If ixsub > 0, receive data for left y-line of uext (via bufleft). */
  if (ixsub != 0) {
    MPI_Wait(&request[2],&status);

    /* Copy the buffer to uext. */
    for (ly = 0; ly < MYSUB; ly++) {
      offsetue = (ly+1)*dsizex2;
      uext[offsetue] = bufleft[ly];
    }
  }

  /* If ixsub < NPEX-1, receive data for right y-line of uext (via bufright). */
  if (ixsub != NPEX-1) {
    MPI_Wait(&request[3],&status);

    /* Copy the buffer to uext */
    for (ly = 0; ly < MYSUB; ly++) {
      offsetue = (ly+2)*dsizex2 - 1;
      uext[offsetue] = bufright[ly];
    }
  }
} /* End of BRecvWait. */


/*******************************************************************
 * PrecondHeateq: setup for diagonal preconditioner for heatsk.    *
 *                                                                 *
 * The optional user-supplied functions PrecondHeateq and          *
 * PSolveHeateq together must define the left preconditoner        *
 * matrix P approximating the system Jacobian matrix               *
 *                   J = dF/du + cj*dF/du'                         *
 * (where the DAE system is F(t,u,u') = 0), and solve the linear   *
 * systems P z = r.   This is done in this case by keeping only    *
 * the diagonal elements of the J matrix above, storing them as    *
 * inverses in a vector pp, when computed in PrecondHeateq, for    *
 * subsequent use in PSolveHeateq.                                 *
 *                                                                 *
 * In this instance, only cj and data (user data structure, with   * 
 * pp etc.) are used from the PrecondHeateq argument list.         *
 ******************************************************************/
  
int PrecondHeateq(integer local_N, real tt, N_Vector yy,
                  N_Vector yp, N_Vector rr, real cj,
                  ResFn res, void *rdata, void *pdata,
                  N_Vector ewt, N_Vector constraints, real hh, 
                  real uround, long int *nrePtr,
                  N_Vector tempv1, N_Vector tempv2, N_Vector tempv3)
{
  integer i, j, offset, loc;
  real *rv, *zv, *ppv, pelinv, pel;
  integer lx, ly, ixbegin, ixend, jybegin, jyend, locu, mxsub, mysub;
  integer ixsub, jysub, npex, npey;
  UserData data;
  real p1, p2;

  data = (UserData) pdata;
  p1 = data->p[0];
  p2 = data->p[1];

  ppv = N_VDATA(data->pp);
  ixsub = data->ixsub;
  jysub = data->jysub;
  mxsub = data->mxsub;
  mysub = data->mysub;
  npex  = data->npex;
  npey  = data->npey;
  
  /* Initially set all pp elements to one. */
  N_VConst(ONE, data->pp);

  /* Prepare to loop over subgrid. */
  ixbegin = 0;
  ixend   = mxsub-1;
  jybegin = 0;
  jyend   = mysub-1;
  if (ixsub == 0) ixbegin++; if (ixsub == npex-1) ixend--;
  if (jysub == 0) jybegin++; if (jysub == npey-1) jyend--;
  pel = cj + ((p1*data->coeffx) + (p2*data->coeffy))*TWO;
  pelinv = ONE/pel;

  /* Load the inverse of the preconditioner diagonal elements
     in loop over all the local subgrid.                           */

  for (ly = jybegin; ly <=jyend; ly++) {
    for (lx = ixbegin; lx <= ixend; lx++) {
      locu  = lx + ly*mxsub;
      ppv[locu] = pelinv;
    }
  }

  return(SUCCESS);

} /* End of PrecondHeateq. */


/******************************************************************
 * PSolveHeateq: solve preconditioner linear system.              *
 * This routine multiplies the input vector rvec by the vector pp *
 * containing the inverse diagonal Jacobian elements (previously  *
 * computed in PrecondHeateq), returning the result in zvec.      */
  
int PSolveHeateq(integer local_N, real tt, N_Vector uu,
                 N_Vector up, N_Vector rr, real cj, ResFn res, void *rdata,
                 void *pdata, N_Vector ewt, real delta, N_Vector rvec,
                 N_Vector zvec, long int *nrePtr, N_Vector tempv)
 {
  UserData data;

  data = (UserData) pdata;

  N_VProd(data->pp, rvec, zvec);

  return(SUCCESS);

} /* End of PSolveHeateq. */
\end{verbatim}

Sample output for the example program {\tt sensheatpk}. \\

\begin{verbatim}
sensheatpk: Heat equation, parallel example problem for SensIDA 
            Discretized heat equation on 2D unit square. 
            Zero boundary conditions, polynomial initial conditions.
            Mesh dimensions: 10 x 10            Total system size: 100

Subgrid dimensions: 5 x 5            Processor array: 2 x 2
Number of sensitivities: Ns = 2
Parameter values:       p_1 =  1.00e+00,    p_2 =  1.00e+00
Scale factors:       pbar_1 =  1.00e+00, pbar_2 =  1.00e+00
Finite difference:   rhomax = 0
Tolerance parameters:  rtol = 0,  atol = 0.001
Constraints set to force all components of solution u >= 0. 
iopt[SUPPRESSALG] = 1 to suppress local error testing on all boundary components. 
Linear solver: IDASPGMR  Preconditioner: diagonal elements only.

Output Summary:   max(u) = max-norm of solution u 
                max(s_i) = max-norm of sensitivity vector s_i

  time       max(u)     k   nst  nni  nli   nre    h       npe nps
 .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .
  0.00     9.75461e-01  0    0    1    2     5  1.00e-05    2  11
max(s_1) = 0.00000e+00
max(s_2) = 0.00000e+00

  0.01     8.24106e-01  2   11   14   25    41  2.56e-03   10  73
max(s_1) = 7.20774e-02
max(s_2) = 7.20774e-02

  0.02     6.88134e-01  3   14   18   40    60  5.12e-03   10 100
max(s_1) = 1.27129e-01
max(s_2) = 1.27129e-01

  0.04     4.70846e-01  3   18   22   58    82  5.12e-03   10 130
max(s_1) = 1.81792e-01
max(s_2) = 1.81792e-01

  0.08     2.16343e-01  3   22   27   94   123  1.02e-02   11 181
max(s_1) = 1.68497e-01
max(s_2) = 1.68494e-01

  0.16     4.54871e-02  4   30   36  147   185  1.02e-02   11 261
max(s_1) = 7.05468e-02
max(s_2) = 7.05478e-02

  0.32     2.00938e-03  2   38   47  226   275  4.10e-02   13 373
max(s_1) = 5.95024e-03
max(s_2) = 5.94397e-03

  0.64     2.04003e-04  1   44   56  270   328  1.47e-01   15 444
max(s_1) = 4.18633e-04
max(s_2) = 4.18349e-04

  1.28     3.24684e-04  1   47   62  283   347  2.95e-01   19 475
max(s_1) = 3.74875e-04
max(s_2) = 3.79646e-04

  2.56     3.16884e-04  1   49   65  293   360  5.90e-01   20 494
max(s_1) = 5.97687e-04
max(s_2) = 5.87800e-04

  5.12     4.80339e-05  1   51   68  299   369  2.36e+00   22 509
max(s_1) = 4.89528e-04
max(s_2) = 4.51301e-04

 10.24     3.54902e-04  1   52   70  307   379  4.72e+00   23 523
max(s_1) = 1.84619e-04
max(s_2) = 1.57882e-04


 netf = 0,   ncfn = 1,   ncfl = 0 

\end{verbatim}
\end{document}
