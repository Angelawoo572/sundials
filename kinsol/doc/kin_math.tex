%===================================================================================
\chapter{Mathematical Considerations}\label{s:math}
%===================================================================================

{\kinsol} solves nonlinear algebraic systems in real $N$-space, which
we write as \index{nonlinear system!definition|)}
\begin{equation}\label{nonlinear_system}
  F(u) = 0 \, , \quad F:\mbox{\bf R}^N \rightarrow \mbox{\bf R}^N \, ,
\end{equation}
given an initial guess $u_0$. \index{nonlinear
system!definition|(}

{\kinsol} can employ either the Inexact Newton method developed in
\cite{Bro:87,BrSa:90,DES:82} and further described in
\cite{DeSc:96,Kel:95}, or a Modified Newton method resulting in the
following iteration scheme:

\index{Inexact Newton iteration!definition|)}
\index{Modified Newton iteration!definition|)} \vspace{1ex}
\noindent{\bf Newton iteration}
\begin{enumerate}
   \item Set $u_0 = $ an initial guess
   \item For $n = 0, 1, 2,...$ until convergence do:
      \begin{itemize}
          \item[(a)] Solve $J(u_n)\delta_n = -F(u_n)$ \label{e:Newton} \\
                     (inexact) iterative method \\
	             (modified) direct method
          \item[(b)] Set $u_{n+1} = u_n + \lambda \delta_n$,
          $0 < \lambda \leq 1$
          \item[(c)] Test for convergence
      \end{itemize}
\end{enumerate}
Here, $u_n$ is the $n$th iterate to $u$, and $J(u) = F'(u)$ is the system
Jacobian. At each stage in the iteration process, a scalar multiple of the approximate
solution, $\delta_n$, is added to $u_n$ to produce a new iterate, $u_{n+1}$.
A test for convergence is made before the iteration continues.

For solving the linear system given in step 2(a), {\kinsol} provides
a choice of five methods:
\begin{itemize}
\item a dense direct solver (serial version only),
\item a band direct solver (serial version only),
\item {\spgmr} = Scaled Preconditioned GMRES,
\item {\spbcg} = Scaled Preconditioned Bi-CGStab, or
\item {\sptfqmr} = Scaled Preconditioned TFQMR.
\end{itemize}

In the cases of the direct solver (dense or band), the iteration is a Modified
Newton iteration, in that the elements of the coefficient matrix are manipulated
directly through matrix decomposition. However, for any of the preconditioned Krylov
subspace methods (GMRES, Bi-CGStab, or TFQMR), it is an Inexact Newton iteration,
in which the iteration matrix is applied in a matrix-free manner, with matrix-vector
products $Jv$ obtained by either finite difference quotients or a user-supplied routine.
The system Jacobian $J$ (direct cases) or preconditioner matrix $P$ (Krylov cases)
is updated as infrequently as possible to balance the high costs of matrix
operations against other costs. Specifically, this matrix update occurs when:
\begin{itemize}
\item starting the problem (optional),
\item $\|\lambda\delta_{n-1}\|_{D_u,\infty} > 1.5$ (inexact only),
\item 10 nonlinear iterations have passed since the last update,
\item the linear solver failed recoverably with outdated Jacobian information,
\item the global strategy failed with outdated Jacobian information, or
\item $\|\lambda\delta_{n}\|_{D_u,\infty} < $ {\sc steptol} with outdated Jacobian
information.
\end{itemize}
However, the direct solvers can also optionally use a nonlinear residual monitoring
scheme to control when the system Jacobian is updated. Specifically, this update
will occur when 5 nonlinear iterations have passed since the last update and
\[ \|D_{u}F(u_n)\|_2 > \omega \|D_{u}F(u_m)\|_2 \, , \]
where $u_n$ is the current iterate and $u_m$ is the iterate from the last nonlinear
iteration when the system Jacobian was updated. In the above, $\omega$ is a scalar
given by
\begin{equation}
\omega = \min \left (\omega_{min} \, e^{\max \left ( 0 , \beta - 1 \right )} , \omega_{max}\right ) \, ,
\end{equation}
where $\beta$ is defined as
\begin{equation}
\beta = \frac{\| D_{u}F(u_n) \|_2}{\mbox{{\sc ftol}}} \, ,
\end{equation}
where {\sc ftol} is an input scalar tolerance discussed later.

Regarding the iterative linear methods, in the case where finite differences
are used to approximate the matrix-vector product $Jv$, the approximation is
of the form given by
\begin{equation}\label{jacobv}
J(u) v \approx [F(u+\sigma v) - F(u)]/\sigma \,
\end{equation}
where $u$ is the current approximation to a root of
(\ref{nonlinear_system}), and $\sigma$ is a scalar. The choice of
$\sigma$ is taken from \cite{BrSa:90} and is given by
\begin{equation}\label{sigma_comp}
  \sigma = \frac{\max \{|u^T v|, \mbox{typ}u^T |v|\}}{\|v\|_2}
  \mbox{sign}(u^T v) \sqrt{U} \, ,
\end{equation}
where $\mbox{typ}u$ is a vector of typical values for the absolute
values of the solution (and can be taken to be inverses of the
scale factors given for $u$ as described below), and $U$ is unit
roundoff. Convergence of the Newton method is maintained as long
as the value of $\sigma$ remains appropriately small as shown in
\cite{Bro:87}.

To the above methods are added preconditioning (iterative only)
and scaling. Scaling is allowed for both the solution vector and
the system function vector. For scaling to be used, the user should
supply values $D_u$, which are diagonal elements of the scaling matrix
such that $D_u u_n$ has all components roughly the same magnitude
when $u_n$ is close to a solution, and $D_F$, which are diagonal
scaling matrix elements such that $D_F F$ has all components
roughly the same magnitude when $u_n$ is not too close to a
solution. In the text below, we use the following scaled norms:
\begin{equation}
\|z\|_{D_u} = \|D_u z\|_2, \;\; \|z\|_{D_F} = \|D_F z\|_2, \;\;
\|z\|_{D_u,\infty} = \|D_u z\|_{\infty}, \;\; {\rm and} \;\;
\|z\|_{D_F,\infty} = \|D_F z\|_{\infty}
\end{equation}
where $\| \cdot \|_{\infty}$ is the max norm.  When scaling values
are provided for the solution vector, these values are
automatically incorporated into the calculation of $\sigma$ in
(\ref{sigma_comp}). Additionally, right preconditioning is
provided if the preconditioning setup and solve routines are
supplied by the user. In this case, the iterative method (GMRES, Bi-CGStab,
or TFQMR) is applied to the linear systems $(JP^{-1})(P\delta) = -F$, where
$P$ denotes the right preconditioning matrix.

Two methods of applying a computed step $\delta_n$ to the
previously computed solution vector are implemented. The first and
simplest is the standard Newton strategy which applies step 2(b) as
above with $\lambda$ always set to $1$. The other method is a
global strategy, which attempts to use the direction implied by
$\delta_n$ in the most efficient way for furthering convergence of
the nonlinear problem. This technique is implemented in the second
strategy, called Linesearch.  This option employs both the
$\alpha$ and $\beta$ conditions of the Goldstein-Armijo linesearch
given in \cite{DeSc:96} for step 2(b), where $\lambda$ is chosen
to guarantee a sufficient decrease in $F$ relative to the step
length as well as a minimum step length relative to the initial
rate of decrease of $F$.  One property of the algorithm is that
the full Newton step tends to be taken close to the solution.  For
more details, the reader is referred to \cite{DeSc:96}.

Stopping criteria for the Newton method are applied to
both of the nonlinear residual and the step length.  For the
former, the Newton iteration must pass a stopping test
\[ \|F(u_n)\|_{D_F,\infty} < \mbox{\sc ftol} \, , \]
where {\sc ftol} is an input scalar tolerance with a default value
of $U^{1/3}$. For the latter, the Newton method will terminate
when the maximum scaled step is below a given tolerance
\[ \|\lambda\delta_n\|_{D_u,\infty} < \mbox{\sc steptol} \, , \]
where {\sc steptol} is an input scalar tolerance with a default
value of $U^{2/3}$.  Only the first condition (small residual)
is considered a successful completion of {\kinsol}.  The second
condition (small step) may indicate that the iteration is stalled
near a point for which the residual is still unacceptable.

When using an iterative method, three options for stopping criteria
for the linear system solve are available, including the two algorithms
of Eisenstat and Walker \cite{EiWa:96}. The Krylov iteration must pass
a stopping test
\[ \|J \delta_n + F\|_{D_F} < (\eta_n + U) \|F\|_{D_F} \, , \]
where $\eta_n$ is one of:
\begin{itemize}
\item Eisenstat and Walker Choice 1
  \[
  \eta_n = \frac{\left|\; \|F(u_n)\|_{D_F}
      - \|F(u_{n-1}) + J(u_{n-1}) \delta_n \|_{D_F}
      \; \right|}
  {\|F(u_{n-1})\|_{D_F}} \, ,
  \]
\item Eisenstat and Walker Choice 2
  \[
  \eta_n = \gamma
  \left( \frac{ \|F(u_n)\|_{D_F}}{\|F(u_{n-1})\|_{D_F}} \right)^{\alpha} \, ,
  \]
where default values of $\gamma$ and $\alpha$ are $0.9$ and $2$,
 respectively.
\item  $\eta_n$ = constant with 0.1 as the default.
\end{itemize}
The default is Eisenstat and Walker Choice 1. For both options 1
and 2, appropriate safeguards are incorporated to ensure that
$\eta$ does not decrease too quickly~\cite{EiWa:96}.

As a user option, {\kinsol} permits the application of inequality
constraints, $u^i > 0$ and $u^i < 0$, as well as $u^i \geq 0$ and
$u^i \leq 0$, where $u^i$ is the $i$th component of $u$.  Any such
constraint, or no constraint, may be imposed on each component.
{\kinsol} will reduce step lengths in order to ensure that no
constraint is violated.  Specifically, if a new Newton iterate
will violate a constraint, the maximum (over all $i$) step length
along the Newton direction that will satisfy all constraints is
found and $\delta_n$ in Step 2(b) is scaled to take a step of that
length.


