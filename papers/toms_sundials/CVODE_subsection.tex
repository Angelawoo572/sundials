\subsection{CVODE}

CVODE solves ODE initial value problems in real $N$-space, which we
write in the abstract form
\begin{equation}
  \dot{y} = f(t,y),~~ y(t_0) = y_0,~~ y \in \mbox{\bf R}^N ~.
\label{ODE} \end{equation}
Here we use $\dot{y}$ to denote $dy/dt$.  While we use $t$ to denote
the independent variable, and usually this is time, it certainly need
not be.  CVODE solves both stiff and nonstiff systems.  Roughly
speaking, stiffness is characterized by the presence of at least one
rapidly damped mode, whose time constant is small compared to the time
scale of the solution itself.

The methods used in CVODE are variable-order, variable-step multistep
methods, based on formulas of the form
\begin{equation}
 \sum_{i = 0}^{K_1} \alpha_{n,i} y_{n-i} + 
     h_n \sum_{i = 0}^{K_2} \beta_{n,i} \dot{y}_{n-i} = 0 ~.
\label{LMM} \end{equation}
Here the $y_n$ are computed approximations to $y(t_n)$, and
$h_n = t_n - t_{n-1}$ is the step size.  The user of CVODE must choose
appropriately one of two multistep methods: For nonstiff problems,
CVODE includes the Adams-Moulton formulas, characterized by $K_1 = 1$
and $K_2 = q$ above, where the order $q$ varies between $1$ and $12$.
For stiff problems, CVODE includes the Backward Differentiation
Formulas (BDFs) in so-called fixed-leading coefficient form, given by
$K_1 = q$ and $K_2 = 0$, with order $q$ varying between $1$ and $5$.
The coefficients are uniquely determined by the method type, its
order, the recent history of the stepsizes, and the normalization
$\alpha_{n,0} = -1$.

For either choice of formula, the nonlinear system
\begin{equation}
G(y_n) \equiv y_n - h_n \beta_{n,0} f(t_n,y_n) - a_n = 0
  \mbox{~,~~~where~~~} a_n \equiv  
  \sum_{i>0}(\alpha_{n,i} y_{n-i} + h_n \beta_{n,i} \dot{y}_{n-i}),
\label{NLS} \end{equation}
must be solved (approximately) at each time step.  For this, CVODE
offers the choice of either {\em functional iteration}, suitable only
for nonstiff systems, and various versions of {\em Newton iteration}.
Functional iteration involves evaluations of $f$ only, while Newton
iteration requires the solution of linear systems
\begin{equation} M (y_{n(m+1)} - y_{n(m)}) = -G(y_{n(m)}) ~,
\label{Newtoncorr} \end{equation}
in which
\begin{equation} M \approx I - \gamma J, ~~~J = \partial f / \partial y 
                 ~~~ \mbox{and} ~~~\gamma = h_n \beta_{n,0} ~. 
\label{Newtonmat} \end{equation}
For the Newton corrections, CVODE provides a choice of four methods:
\begin{itemize}
\item a dense direct solver (serial version only),
\item a band direct solver (serial version only),
\item a diagonal approximate Jacobian solver, or
\item SPGMR = Scaled Preconditioned GMRES, without restarts.
\end{itemize}
For large stiff systems, where direct methods are not feasible, the
combination of a BDF integrator and a preconditioned GMRES algorithm
yields a powerful tool, because it combines established methods for
stiff integration, nonlinear iteration, and Krylov (linear) iteration
with a problem-specific treatment of the dominant source of stiffness,
in the form of the user-supplied preconditioner matrix \cite{BrHi:89}.

In the cases of a direct solver (dense, band, or diagonal), the
iteration is a Modified Newton iteration, in that the iteration matrix
$M$ is fixed throughout the nonlinear iterations.  However, for SPGMR,
it is an Inexact Newton iteration, in which $M$ is applied in a
matrix-free manner, with matrix-vector products $Jv$ obtained by
either difference quotients or a user-supplied routine.  The matrix
$M$ (direct cases) or preconditioner matrix $P$ (SPGMR case) is
updated as infrequently as possible, to balance the high costs of
matrix operations against other costs.  Specifically, this matrix
update occurs when:
\begin{itemize}
\item starting the problem,
\item more than 20 steps have been taken since the last update,
\item the value $\bar{\gamma}$ of $\gamma$ at the last update
satisfies $|\gamma/\bar{\gamma} - 1| > .3$,
\item a non-fatal convergence failure just occurred, or
\item an error test failure just occurred.
\end{itemize}
When forced by a convergence failure, an update of $M$ or $P$ may or
may not involve a re-evaluation of $J$ (in $M$) or of Jacobian data
(in $P$), depending on whether Jacobian error was the likely cause of
the failure.  More generally, the decision is made to re-evaluate $J$
(or instruct the user to re-evaluate Jacobian data in $P$) when:
\begin{itemize}
\item starting the problem,
\item more than 50 steps have been taken since the last evaluation,
\item a convergence failure occurred with an outdated matrix, and
the value $\bar{\gamma}$ of $\gamma$ at the last update
satisfies $|\gamma/\bar{\gamma} - 1| < .2$, or
\item a convergence failure occurred that forced a stepsize reduction.
\end{itemize}

The stopping test for the Newton iteration is related to the
subsequent local error test, with the goal of keeping the nonlinear
iteration errors from interfering with local error control.  As
described below, the final computed value $y_n$ will have to satisfy a
local error test $\| y_n - y_{n(0)} \| \leq \epsilon$.  Here, we want
to insure that the iteration error $y_n - y_{n(m)}$ is small relative
to $\epsilon$, specifically that it is less that $.1 \epsilon$.
For this, we also estimate the linear convergence rate constant $R$ as
follows: We initialize $R$ to 1, and reset $R = 1$ when $M$ or $P$ is
updated.  After computing a correction $\delta_m = y_{n(m)}-y_{n(m-1)}$,
we update $R$ if $m > 1$ as
\[ R \leftarrow \max\{0.3R , \|\delta_m\| / \|\delta_{m-1}\| \} ~. \]
Now we use the estimate
\[ \| y_n - y_{n(m)} \| \approx \| y_{n(m+1)} - y_{n(m)} \| 
   \approx R \| y_{n(m)} - y_{n(m-1)} \|  =  R \|\delta_m \| ~. \]
Therefore the convergence (stopping) test is 
\[  R \|\delta_m \| < 0.1 \epsilon ~. \]
We allow at most 3 Newton iterations, and declare the iteration
diverged if any $\|\delta_m\| / \|\delta_{m-1}\| > 2$.  If convergence
fails with $J$ or $P$ current, we are forced to reduce the stepsize
$h$, and we replace $h$ by $h/4$.

When SPGMR is used to solve the linear system, its errors must also be
controlled, and this also involves the local error test constant.  The
linear iteration error in the solution vector $\delta_m$ is
approximated by the preconditioned residual vector.  Thus to insure
(or attempt to insure) that the linear iteration errors do not
interfere with the nonlinear error and local integration error
controls, we require that the norm of the preconditioned residual
in SPGMR is less than $.05 \cdot .1 \epsilon$.

With the direct dense and band methods, the Jacobian may be supplied
by a user routine, or approximated by difference quotients,
at the user's option.  In the latter case, we use the usual
approximation
\[ J_{ij} = [f^i(t,y+\sigma_j e_j) - f^i(t,y)]/\sigma_j ~. \]
The increments $\sigma_j$ are given by
\[ \sigma_j = max(\sqrt{U} |y^j| , \sigma_0 W_j) ~, \]
where $U$ is the unit roundoff, $\sigma_0$ is a dimensionless value,
and $W_j$ is the error weight for $y^j$ set in terms of input
tolerances.  In the dense case, this scheme requires $N$ evaluations
of $f$, one for each column of $J$.  In the band case, the columns
of $J$ are computed in groups, by the Curtis-Powell-Reid algorithm,
with a number of $f$ evaluations equal to the bandwidth.

In the case of SPGMR, preconditioning may be used on the left, on the
right, or both, with user-supplied routines for the preconditioning
setup and solve operations, and optionally also for the matrix-vector
products $Jv$ required.  If a routine for $Jv$ is not supplied, these
products are computed as
\[ Jv = [f(t,y+\sigma v) - f(t,y)]/\sigma ~. \]
The increment $\sigma$ is $1/\|v\|_{WRMS}$, where $\|\cdot\|_{WRMS}$
is the weighted RMS norm involving the error weights $W_j$.

Regardless of the multistep method or nonlinear iteration chosen,
CVODE integrates the initial value problem with local error control
and variation of both step size and order.  A weighted root-mean-square
norm of the estimated local is made to satisfy a local error test on
every step, using weights that involve relative and absolute
tolerances provided by the user.  Based primarily on these estimated
local errors, at both actual order and contemplated nearby orders, the
stepsize $h_n$ and the order $q$ are frequently varied, always in an
attempt to maximize the stepsize, subject to the local error test.

The various algorithmic features of CVODE, as inherited from VODE and
VODPK, are documented in Refs. \cite{BBH:89}, \cite{Byr:92}, and
\cite{Hin:00}.  A full description of the usage of CVODE is given in
\cite{CoHi:94} and \cite{ByHi:98}.

There is an important additional part of the CVODE order selection
algorithm that is not based on local error, but instead provides
protection against potentially unstable behavior of the BDF methods.
At order 1 or 2, the BDF method is A-stable.  But at orders 3 to 5 it
is not, and the region of instability includes a portion of the left
half-plane that is concentrated near the imaginary axis.  The size of
that region of instability grows as the order increases from 3 to 5.
What this means is that, when running BDF at these higher orders, if
an eigenvalue $\lambda$ of the system lies close enough to the
imaginary axis, the step sizes $h$ for which the method is stable are
limited (at least according to the linear stability theory) to a set
that prevents $h\lambda$ from leaving the stability region.  System
eigenvalues that are likely to cause this instability are ones that
correspond to weakly damped oscillations, such as might arise from a
semi-discretized advection-diffusion PDE with advection dominating
over diffusion.

CVODE includes an optional algorithm called STALD (STAbility Limit
Detection), which attempts to detect directly the presence of a
stability region boundary that is limiting the step sizes in the
presence of a weakly damped oscillation \cite{Hin:92}.  Working
directly with history data that is readily available, if it concludes
that the step size is in fact stability-limited, it dictates a
reduction in the method order, regardless of the outcome of the
error-based algorithm.  STALD has been tested in combination with the
VODE solver on linear advection-dominated advection-diffusion problems
\cite{Hin:95}, where it works well.  The implementation in CVODE has
been successfully tested on linear and nonlinear advection-diffusion
problems, among others.
The STALD option adds some overhead computational cost to the CVODE
solution.  In timing tests, these overhead costs have ranged from 2\%
to 7\% of the total, depending on the size and complexity of the
problem, with lower relative costs for larger problems.  Therefore, it
should be activated only when there is reasonable expectation of modes
in the user's system for which it is appropriate, together with poor
performance at orders 3-5, for no apparent reason, with the option
turned off.
