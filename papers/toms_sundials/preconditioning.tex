\section{Preconditioning}

Whenever a Krylov method is used for the linear systems within a
Newton iteration, it is generally necessary to precondition the system
in order to obtain acceptable efficiency.  To be effective, the
preconditioner matrix must in some sense approximate the relevant
matrix (Jacobian matrix or Newton matrix), and yet be reasonably
efficient to evaluate and solve.  Typical preconditioners used with
SUNDIALS solvers are based on approximations to the Jacobian matrices
of the systems involved.  Because the Krylov iteration occurs within a
Newton iteration, and often also within a time integration, and each
of these iterations has its own test for convergence, the
preconditioner may use a very crude approximation, as long as it
captures the dominant numerical feature(s) of the system.  
We have found that the combination of a preconditioner with the
Newton-Krylov iteration, using even a fairly poor approximation to
the Jacobian, can be surprisingly superior to using the same matrix
without Krylov acceleration (i.e. a modified Newton iteration), as
well to using the Newton-Krylov method with no preconditioning.

We further exploit this nested iteration setting, and differences in
the costs of the various preconditioner operations, by treating the
preconditioner matrix $P$ in two separate phases:
\begin{itemize}
\item a setup phase: evaluate and preprocess $P$ (done infrequently), and
\item a solve phase: solve systems $Px = b$ (done frequently).
\end{itemize}
Accordingly, the user of each SUNDIALS solver must supply two separate
routines for these operations.  The setup of $P$ is generally more
expensive than the solve operation, and so it is done as infrequently
as possible, with updates to $P$ dictated primarily by convergence
failures of the Newton iteration.  The system solves $Px = b$ must of
course be done at every Krylov iteration.

We provide help to SUNDIALS users with respect to preconditioning in
two ways.  First, for each solver, there is at least one example
problem program which illustrates a preconditioner for
reaction-diffusion systems, based on the concept of operator
splitting.  The example does not perform operator splitting (which
generally requires giving up error control), but builds the
preconditioner from one of two operators (reaction) in the problem.
These examples are intended to serve as templates for possible
user-defined preconditioners in similar applications.  See
Ref. \cite{BrHi:89} for an extensive study of preconditioners for
reaction-transport systems.

Second, the SUNDIALS package includes some extra preconditioner
modules, for optional use with the solvers.  For the parallel version
of each of the basic solvers, we provide a preconditioner module which
generates a band-block-diagonal (BBD) preconditioner.  With serial
CVODE, we also supply a band preconditioner module.  These band and
BBD preconditioners are described below.  Full details on the usage
of these optional modules are given in the respective user guides
--- \cite{ByHi:98},\cite{TaHi:98},\cite{HiTa:99}.

In any case, for any given choice of the approximate Jacobian, it may
be best to consider choices for the preconditioner linear solver that
are more appropriate to the specific problem than those supplied with
SUNDIALS.

\subsection{Preconditioners for CVODE}

Assuming that the CVODE user has chosen one of the stiff system
options, recall from Eq. (\ref{Newtonmat}) that the Newton matrix for
the nonlinear iteration has the form $I - \gamma J$, where $J$ is the
ODE system Jacobian $J = \partial f / \partial y$.  Therefore, a
typical choice for the preconditioner matrix $P$ is
\[ P = I - \gamma \tilde{J} ~, \mbox{ with } \tilde{J} \approx J ~. \]
As noted above, the approximation may be a crude one.  

The setup phase for $P$ is generally performed only once every several
time steps, in an attempt to minimize costs.  In addition to
evaluating $P$, it may involve preprocessing operations, such as LU
decomposition, suitable for later use in the solve phase.  Within the
setup routine, the user can save and reuse the relevant parts of the
approximate Jacobian $\tilde{J}$, as directed by CVODE (in its call to
the user routine), so as to further reduce costs when the scalar
$\gamma$ has changed since the last setup call.  This requires the
user to manage the storage of the saved data involved.  But this
tradeoff of storage for potential savings in computation may be
beneficial if the cost of evaluating $\tilde{J}$ is significant in
comparison with the other operations performed on $P$.

With the serial version of CVODE, we supply a preconditioner module
called CVBANDPRE, whose use is optional.  This computes and solves a
banded approximation $P$ to the Newton matrix, computed with
difference quotient approximations.  The user supplies a pair of lower
and upper half-bandwidths --- {\tt ml,mu} --- that define the shape of
the approximate Jacobian $\tilde{J}$; its full bandwidth is
{\tt ml+mu+1}.  $\tilde{J}$ is computed using difference quotients,
with {\tt ml+mu+1} evaluations of $f$.  The true Jacobian need not be
banded, or its true bandwidth may be larger, as long as $\tilde{J}$
approximates $J$ sufficiently well.

Extending this idea to the parallel setting, CVODE also includes a
module, called CVBBDPRE, that generates a band-block-diagonal
preconditioner.  It is designed for PDE-based problems, and uses the
idea of Domain Decomposition, as follows.  Suppose that a
time-dependent PDE system, with the spatial operators suitably
discretized, yields the ODE system $\dot{y} = f(t,y)$.  Now consider a
decomposition of the (discretized) spatial domain into $M$
non-overlapping subdomains.  This decomposition induces a block form
$y = (y_1,\cdots,y_M)$ for the vector $y$, and similarly for $f$.  We
will use this distribution for the solution with CVODE on $M$
processors.

The $m$-th block of $f$, $f_m(t,y)$, depends on both $y_m$ and ghost
cell data from other blocks $y_{m'}$, typically in a local manner,
according to the discretized spatial operators.  However, when we
build the preconditioner $P$, we will ignore that coupling, and
include only the diagonal blocks $\partial f_m / \partial y_m$.  In
addition, it may be cost-effective to exclude from $P$ some parts of
the function $f$.  Thus for the computation of these blocks, we
replace $f$ by a function $g \approx f$ (and $g = f$ is certainly
allowed).  For example, $g$ may be chosen to have smaller set of ghost
cell data than $f$.  In the CVBBDPRE module, the matrix blocks
$\partial g_m/\partial y_m$ are approximated by band matrices $J_m$,
again exploiting the local spatial coupling, and on processor $m$
these are computed by a difference quotient scheme.  Then the complete
preconditioner is given by
\[ P = diag[P_1,\cdots,P_M] ~,~~~ P_m = I_m - \gamma J_m ~. \]
Linear systems $Px = b$ are then solved by banded LU and backsolve
operations on each processor.  The setup phase consists of the
evaluation and banded LU decomposition of $P_m$, and the solve phase
consists of a banded backsolve operation.

In order to minimize costs in the difference quotient scheme, the
function $g$ is supplied by the user in the form of two routines: One
routine, called once per $P$ evaluation, performs inter-processor
communication of data needed to evaluate the $g_m$.  The other routine
evaluates $g_m$ on processor $m$, assuming that the communication
routine has already been called.  The banded structure of the problem
is exploited in two different ways.  First, the user supplies a pair of
half-bandwidths --- {\tt ml,mu} --- that define the shape of the
matrix $J_m$.  But the user also supplies a second pair of
half-bandwidths --- {\tt mldq,mudq} --- for use in the difference
quotient scheme, in which $J_m$ is computed by way of 
{\tt mldq+mudq+2} evaluations of $g_m$.  The values {\tt ml,mu} may be
smaller than {\tt mldq,mudq} --- trading lower matrix costs for slower
convergence.  Thus for example, a matrix based on 5-point coupling in
2D ({\tt mldq = mudq = } mesh dimension) might be well approximated by
a tridiagonal matrix ({\tt ml = mu = 1}).  In any case, for the sake
of efficiency, both pairs of half-bandwidths may be less than the true
values for $\partial g_m /\partial y_m$, and both pairs may depend on
$m$.

\subsection{Preconditioners for KINSOL and IDA}

The parallel version of the KINSOL package includes a module, called
KINBBDPRE, that provides a band-block-diagonal preconditioner,
analogous to that of the CVODE module CVBBDPRE.  Here the problem to
be solved is $F(u) = 0$, and the preconditioner is constructed by way
of a function $g \approx F$.  Namely, it is defined as
\[ P = diag[P_1,\cdots,P_M] ~,~~
P_m \approx \partial g_m / \partial u_m ~, \]
in terms of the blocks of $g$ and $u$ on processor $m$.  Again, $P_m$
is banded and is computed using difference quotients, with user-supplied
half-bandwidths for both the difference quotient scheme and the
retained band matrix.

Likewise, the parallel version of the IDA package includes a
band-block-diagonal preconditioner module, called IDABBDPRE.  For the
problem $F(t,y,y') = 0$, the preconditioner is defined by way of a
function $G \approx F$.  Specifically, the preconditioner is
\[ P = diag[P_1,\cdots,P_M] ~,~~~
   P_m \approx \partial G_m / \partial y_m
        + \alpha \partial G_m / \partial y'_m ~. \]
Each block $P_m$ is banded, computed using difference quotients, with
user-supplied half-bandwidths for the difference quotient scheme and
the retained matrix.

