\subsection{IDA}\label{ss:IDA}

% What does IDA do?
%------------------
The IDA code is a C implementation of a previous code, DASPK, a DAE
system solver written in Fortran by Petzold, Brown, and Hindmarsh
\cite{BHP:94,BCP:96}.  IDA solves the initial value problem for 
a DAE system of the general form
\begin{equation}\label{e:DAE}
  F(t,y,{\dot y}) = 0 \, ,
  \quad y(t_0) = y_0 \, ,~ {\dot y}(t_0) = {\dot y}_0 \, ,
\end{equation}
where $y$, ${\dot y}$, and $F$ are vectors in ${\bf R}^N$, $t$ is the independent
variable, ${\dot y} = dy/dt$, 
and initial conditions $y(t_0) = y_0$, ${\dot y}(t_0) = {\dot y}_0$ 
are given.  (Often $t$ is time, but it certainly need not be.)

% Initial condition calculation
%------------------------------
Prior to integrating a DAE initial-value problem, an important requirement 
is that the pair of vectors $y_0$ and ${\dot y_0}$ are both initialized to
satisfy the DAE residual $F(t_0,y_0, {\dot y}_0) = 0$.
For a class of problems that includes so-called
semi-explicit index-one systems, IDA provides a routine that computes
consistent initial conditions from a user's initial guess~\cite{BHP:98}.  
For this, the user must identify subvectors of $y$
(not necessarily contiguous), denoted $y_d$ and $y_a$, which are its
differential and algebraic parts, respectively, such that $F$ depends
on ${\dot y}_d$ but not on any components of ${\dot y}_a$.  The assumption that
the system is ``index-one'' means that for a given $t$ and $y_d$, the
system $F(t,y,{\dot y}) = 0$ defines $y_a$ uniquely.  In this case, a solver
within IDA computes $y_a$ and ${\dot y}_d$ at $t = t_0$, given $y_d$ and an
initial guess for $y_a$.  A second available option with this solver
also computes all of $y(t_0)$ given ${\dot y}(t_0)$; this is intended mainly
for quasi-steady state problems, where ${\dot y}(t_0) = 0$ is given.
In both cases, IDA solves the system $F(t_0,y_0, {\dot y}_0) = 0$ for the
unknown components of $y_0$ and ${\dot y}_0$, using Newton iteration
augmented with a linesearch global strategy.  In doing this, it makes
use of the existing machinery that is to be used for solving the
linear systems during the integration, in combination with certain
tricks involving the stepsize (which is set artificially for this
calculation).
For problems that do not fall into either of these categories, the
user is responsible for passing consistent values, or risk failure in
the numerical integration.

% Integration method and nonlinear system
%----------------------------------------
The integration method in IDA is variable-order, variable-coefficient
BDF, in fixed-leading-coefficient form.  {\sf cite a reference here.} 
The method order ranges from 1 to 5, with the BDF of order $k$
given by the multistep formula
\begin{equation}\label{e:BDF}
  \sum_{i=0}^k \alpha_{n,i}y_{n-i} = h_n {\dot y}_n \, ,
\end{equation}
where $y_n$ and ${\dot y}_n$ are the computed approximations to $y(t_n)$
and ${\dot y}(t_n)$, respectively, and the stepsize is $h_n = t_n - t_{n-1}$.  
The coefficients $\alpha_{n,i}$ are uniquely determined by the order
$k$, and the history of the stepsize.  The application of the BDF
(\ref{e:BDF}) to the DAE system (\ref{e:DAE}) results in a nonlinear
algebraic system to be solved at each step:
\begin{equation}\label{e:DAE_nls}
  G(y_n) \equiv 
  F \left( t_n , \, y_n , \, 
    h_n^{-1} \sum_{i=0}^k \alpha_{n,i}y_{n-i} \right) = 0 \, .
\end{equation}
%
Regardless of the method options, the solution of the nonlinear system
(\ref{e:DAE_nls}) is accomplished with some form of Newton iteration.
This leads to a linear system for each Newton correction, of the form
\begin{equation}\label{e:DAE_Newtoncorr}
  J [y_{n(m+1)} - y_{n(m)}] = -G(y_{n(m)})  \, , 
\end{equation}
where $y_{n(m)}$ is the $m$-th approximation to $y_n$. 
%
Here $J$ is some approximation to the system Jacobian
\begin{equation}\label{e:DAE_Jacobian}
  J = \frac{\partial G}{\partial y}
  = \frac{\partial F}{\partial y} + 
  \alpha\frac{\partial F}{\partial {\dot y}} \, ,
\end{equation}
where $\alpha = \alpha_{n,0}/h_n$.  The scalar $\alpha$ changes 
whenever the stepsize or method order changes.
%
The linear systems are solved by one of three methods:
\begin{itemize}
\item direct dense solve (serial version only),
\item direct banded solve (serial version only), or
\item SPGMR = Scaled Preconditioned GMRES, with restarts allowed.
\end{itemize}
For the SPGMR case, preconditioning is allowed only on the left
\footnote{Left preconditioning is required to make the norm of the 
linear residual in the Newton iteration meaningful; in general,
$\| J \Delta y + G \|$ is meaningless, since the weights used in 
the WRMS-norm correspond to $y$}, 
so that GMRES is applied to systems $(P^{-1}J)\Delta y = -P^{-1}F$. 

% Newton iteration
%-----------------
In the cases of a direct linear solver (dense or band), the nonlinear 
iteration (\ref{e:DAE_Newtoncorr}) is a Modified Newton iteration, in
that the Jacobian $J$ is fixed (and usually out of date), with
relaxation with respect to $\bar\alpha \neq \alpha$ in $J$. When using
SPGMR as the linear solver, the iteration is an Inexact Newton iteration,
using the current Jacobian (through matrix-free products $Jv$), in 
which the linear residual $J\Delta y + G$ is nonzero but controlled.
The Jacobian matrix $J$ (direct cases) or preconditioner matrix $P$ 
(SPGMR case) is updated when:
\begin{itemize}
\item starting the problem,
\item the value $\bar\alpha$ at the last update is such that
  $\alpha / {\bar\alpha} < 3/5$ or $\alpha / {\bar\alpha} > 3/5$, or
\item a non-fatal convergence failure occured with out of date $J$ or $P$.
\end{itemize}
The above strategy balances the high cost of frequent matrix evaluations
and preprocessing with the slow convergence due to infrequent updates.
To reduce storage costs, on an update, Jacobian information is always
reevaluated from scratch.

% Newton convergence test
%------------------------
Unlike the CVODE/CVODES case, the stopping test for the Newton iteration
in IDA insures that the iteration error $y_n - y_{n(m)}$ is small relative
to $y$ itself. For this, we estimate the linear convergence rate at all 
iterations $m>1$ as
\begin{equation*}
R = \left( \frac{\delta_m}{\delta_1} \right)^{\frac{1}{m-1}} \, , 
\end{equation*}
where the $\delta_m = y_{n(m)} - y_{n(m-1)}$ is the correction at
iteration $m=1,2,\ldots$. The Newton iteration is halted if $R>0.9$.
The convergence test at the $m$-th iteration is then
\begin{equation}\label{e:DAE_nls_test}
S \| \delta_m \| < 0.33 \, ,
\end{equation}
where $S = R/(R-1)$ whenever $m>1$ and $R\le 0.9$. The user has the
option of changing the constant in the convergence test from its default 
value of $0.3$.
%
The quantity $S$ is set to $S=20$ initially and whenever $J$ or $P$ is
updated, and is reset to $S=100$ on a step with $\alpha \neq \bar\alpha$.
Note that at $m=1$, the convergence test (\ref{e:DAE_nls_test}) uses an old 
value for $S$. Therefore, at the first Newton iteration, we make an additional
test and stop the iteration if $\|\delta_1\| < 0.33 \cdot 10^{-4}$
(since such a $\delta_1$ is probably just noise and therefore not appropriate 
for use in evaluating $R$).
%
We allow only a small number (default value 4) of Newton iterations.
If convergence fails with $J$ or $P$ current, 
we are forced to reduce the stepsize $h_n$, and we replace $h_n$ by $h_n/4$.
The integration is halted after a preset number (default value 10)
of convergence failures. Both the maximum allowable Newton iterations
and the maximum nonlinear convergence failures can be changed by the user
from their default values.

When SPGMR is used to solve the linear system, to minimize the effect of
linear iteration errors on the nonlinear and local integration error controls,
we require the preconditioned linear residual to be small relative to the 
allowed error in the Newton iteration; i.e., 
$\| P^{-1}(Jx+G) \| < 0.05 \cdot 0.33$.

% Jacobian DQ approximations
%---------------------------
In the direct cases, the Jacobian $J$ defined in (\ref{e:DAE_Jacobian}) 
can be either supplied by the user or have IDA compute one internally 
by difference quotients. In the latter case, we use the approximation
\begin{gather*}
  J_{ij} = [F^i(t,y+\sigma_j e_j,{\dot y}+\alpha\sigma_j e_j) - 
  F^i(t,y,{\dot y})]/\sigma_j \, , \text{ with}\\
  \sigma_j = \sqrt{U} \max \left\{ |y^j|, 
    \max \left\{ |h{\dot y}^j|,W_j \right\}\right\} sign(h {\dot y}^j)
\end{gather*}
where $U$ is the unit roundoff, $h$ is the current stepsize, and $W_j$ is 
the error weight for $y^j$ set in terms of input tolerances. 
%
In the SPGMR case, if a routine for $Jv$ is not supplied, such products are
approximated with
\begin{equation*}
Jv = [F(t,y+\sigma v,{\dot y}+\alpha\sigma v) - F(t,y,{dot y})]/\sigma \, ,
\end{equation*}
where the increment $\sigma$ is proportional to the square root of the problem 
dimension $N$ (the default value of $1.0$ for the proportionality constant can 
be changed by the user).

% Error control
%--------------
During the course of integrating the system, IDA computes an estimate
$E_n$ of the local truncation error at the $n$-th time step, and
requires this to satisfy the inequality
\begin{equation}\label{Errtest}
  \left\| E_n\right\|_{WRMS} \le 1 \, .               
\end{equation}
% This test imposes tolerances on the local errors by way of the weighted
% root-mean-square norm, which is defined by
% \[
% \left\| E_n\right\|_{WRMS} = \left[ \frac{1}{N} \sum_{i=1}^N 
%     ( E_n^i / w^i ) ^2 \right] ^{1/2} \, .
% \]
% Here a superscript $i$ denotes the $i$-th component, and the $i$-th
% weight is 
% \begin{equation}\label{weights}
% w^i = rtol |y^i| + atol^i ~~~ \mbox{or} ~~~ w^i = rtol |y^i| + atol \, .
% \end{equation}
% This permits an arbitrary combination of relative and absolute error
% control. The user specifies a scalar relative error tolerance $rtol$
% and an absolute error tolerance $atol$ which may be either an
% vector or a scalar (as indicated in (\ref{weights}) above).  
% Since these tolerances define the allowed error per step, they should
% be chosen conservatively.
%
IDA varies both the stepsize $h_n$ and the order $k$ in an attempt to
produce a solution with the minimum number of steps, but always
subject to the local error test (\ref{Errtest}).  After a step at
order $k$, the local truncation errors at orders $k-1$ and (often)
$k+1$ are also estimated, and a change of order is considered on the
basis of the three error norms.  See \cite{BCP:96} for details.  

% Additional constraints on y components
%---------------------------------------
IDA permits the user to impose optional inequality constraints on individual 
components of the solution vector $y$. Any of the following four constraints 
can be imposed: $y^i > 0$, $y^i < 0$, $y^i \geq 0$, or $y^i \leq 0$. 
The constraint satisfaction is tested after a successful nonlinear system solution. 
If any constraint fails, we declare a convergence failure of the Newton iteration 
and reduce the stepsize. Rather than cutting the stepsize by some arbitrary factor, 
IDA estimates a new stepsize $h'$ using a linear approximation of the components 
in $y$ that failed the constraint test (including a safety factor of $0.9$ to 
cover the strict inequality case). These additional constraints are also imposed
during the calculation of consistent initial conditions.

Normally, IDA takes steps until a user-defined output value $t = t_{out}$
is overtaken, and then computes $y(t_{out})$ by interpolation.  However,
a ``one-step'' mode option is available, where control returns to the
calling program after each step.  There are also options to force IDA
not to integrate past a given stopping point $t = t_{stop}$.

